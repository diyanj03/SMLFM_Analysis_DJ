from scipy.stats import linregress
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.lines import Line2D
from scipy.optimize import curve_fit
from scipy.stats.distributions import t
import os
import pandas as pd




def compute_rss_bic(model_func, xdata, ydata, popt):
    """
    Compute Bayesian Information Criterion (BIC) for a fitted model. 
    Assumes residuals are gaussian distributed

    Args:
    - model_func (function): The model function used for fitting.
    - xdata (array-like): Input x data.
    - ydata (array-like): Observed y data.
    - popt (tuple/list): Optimised parameters from the fit.

    Returns:
    - bic (float): Bayesian Information Criterion value.
    """
    n = len(ydata)
    k = len(popt)
    residuals = ydata - model_func(xdata, *popt)
    rss = np.sum(residuals**2)
    bic = n * np.log(rss / n) + k * np.log(n) if rss > 0 else np.nan
    return rss, bic

def compute_all_CI(popt, pcov, n_points, alpha=0.05):
    """
    Compute 95% confidence intervals for all fitted parameters.

    Args:
    - popt (array-like): Optimised parameters from curve_fit.
    - pcov (2D array): Covariance matrix from curve_fit.
    - n_points (int): Number of data points used in the fit.
    - alpha (float): Significance level (default=0.05 for 95% CI).

    Returns:
    - ci_list (list of tuples): [(lower1, upper1), (lower2, upper2), ...] for each parameter.
    """
    n_params = len(popt)
    dof = max(0, n_points - n_params)
    if dof == 0 or np.any(np.isnan(pcov)):
        return [(np.nan, np.nan)] * n_params

    t_crit = t.ppf(1 - alpha/2, dof)
    stderr = np.sqrt(np.diag(pcov))

    ci_list = [(param - t_crit * err, param + t_crit * err)
               for param, err in zip(popt, stderr)]
    return ci_list

def compute_half_width_ci(ci_tuple):
    lower, upper = ci_tuple
    return (upper - lower) / 2

def find_first_valid_index(lst):
    n = len(lst)
    for i in range(n):
        if lst[i] == 0:
            if all(x <= 3 for x in lst[i+1:]):
                return i
        elif lst[i] == 1 and i + 1 < n and lst[i+1] == 1:
            if all(x <= 3 for x in lst[i+2:]):
                return i+1
    return -1

def extract_counts(tracks_dict):
    """
    Args:
    - tracks_dict (dict): Dictionary where keys represent tau_tl (unit: seconds) and values represent path to csv track files generated by SPT.

    Returns:
    - counts_dict (dict): 
        - keys are tau_interval and values represent a tuple:
        - element 1: histograms (counts) for all track lengths for all datasets with the same tau_interval.
        - element 2: total number of counts (sum of all track lengths) for that tau_interval.
        - element 3: number of FOVs from which counts were extracted for that tau_interval.
    """
    counts_dict = {}
    for tau_interval, tracks_folder in tracks_dict.items():
        numPos_concat = []
        numFOVs = 0
        for track_file_name in os.listdir(tracks_folder):
            if track_file_name.endswith('_positionsFramesIntensity.csv'):
                track_file_path = os.path.join(tracks_folder, track_file_name)
                df = pd.read_csv(track_file_path, comment='#')
                numPos_series = df.iloc[:, 1]
                numPos_concat.extend(numPos_series.tolist())
                numFOVs += 1
        
        if not numPos_concat:
            counts = np.array([], dtype='int64')
            total_counts = 0
        else:
            edges = np.arange(min(numPos_concat) - 0.5, max(numPos_concat) + 0.5 + 0.1, 1)
            counts, _ = np.histogram(numPos_concat, bins=edges)
            counts = np.array(counts).astype('int64')
            total_counts = np.sum(counts) 

        counts_dict[tau_interval] = (counts, total_counts, numFOVs)
    return counts_dict


def compute_keffs(counts_dict, plot_semilog):
    """
    Input raw counts for each timelapse interval for calculating keffs.
    Args:
    - counts_dict (dict): Dictionary where keys represent tau_tl (unit: seconds) and values represent a list of raw counts for each frame.

    Returns:
    - keffs_dict (dict): {tau_tl: A_fit, k_eff_fit, (k_eff_CI_lower, k_eff_CI_upper), keff_sd_err}
    - fig (matplotlib object): plot of the exponential fittings
    - fig2 (matplotlib object): plot of the exponential fittings - survival fraction
    """
    
    keff_dict = {}

    # exponential decay to fit data and model k_eff 
    def exp_decay(x, A, k_eff):
        return A * np.exp(-k_eff * x)
   
    fig, ax = plt.subplots()

    for tau_tl, value in counts_dict.items():
        counts = value[0]
        total_counts = np.sum(counts)

        actual_x = np.arange(1, len(counts)+0.01, 1) * tau_tl
        fit_x = np.linspace(min(actual_x), max(actual_x), 200)
        
        counts = np.array(counts)
        valid = counts > 0
        actual_x_valid = actual_x[valid]
        perc_valid = counts[valid]
        log_perc = np.log(perc_valid)
        
        slope, intercept = np.polyfit(actual_x_valid, log_perc, 1)
        k_eff_guess = -slope
        A_guess = np.exp(intercept)
        p0 = [A_guess, k_eff_guess]

        popt, pcov = curve_fit(exp_decay, actual_x, counts, p0=p0)
        A_fit, k_eff_fit = popt
        perr = np.sqrt(np.diag(pcov))
        A_err, k_eff_err = perr
        ci = compute_all_CI(popt, pcov, len(counts))

        keff_dict[tau_tl] = (A_fit, k_eff_fit, ci[1], k_eff_err)
        
        idx_trunc = find_first_valid_index(counts)
        
        perc_surviving = counts / total_counts
        perc_surv_trunc = perc_surviving[:idx_trunc]

        actual_x = actual_x[:idx_trunc]
        fit_x = np.linspace(min(actual_x), max(actual_x), 200)

        scatter = ax.scatter(actual_x, perc_surv_trunc, label=f'{tau_tl}s data',zorder=2)
        
        label = rf"{tau_tl}s fit"

        y_fit = exp_decay(fit_x, A_fit, k_eff_fit) / total_counts

        ax.plot(fit_x, y_fit, label = label, color=scatter.get_facecolors()[0], zorder=1)

    ax.set_xscale('log')
    if plot_semilog:
        ax.set_yscale('log')
        ax.set_xscale('linear')

    ax.set_axisbelow(True)
    ax.grid(which='both', linestyle='--', linewidth=0.5, zorder = 0)
    ax.set_xlabel(r"n$\tau_{\mathrm{tl}}$ (seconds)")
    ax.set_ylabel("fraction surviving")
    ax.set_title('single-exp keff fit')
    ax.legend()

    return keff_dict, fig

def compute_koff(keff_dict, tau_int):
    """
    Weighted linear fit of keff * tau_tl vs tau_tl using keff std errors as weights.

    Args:
    - keff_dict (dict): {tau_tl: (A_fit, keff_fit, keff_CI, keff_sd_err)}
    - tau_int (float): camera integration/exposure time in seconds.

    Returns:
    - rates (tuple): (k_off, k_b) in s^-1
    - fit_metrics (tuple): (r_squared, k_off_std_error)
    - fig (matplotlib figure): plot of weighted linear fit
    """

    def linear_func(x, m, c):
        return m * x + c

    tau_tl_list = np.array(list(keff_dict.keys()))
    keff_values = np.array([v[1] for v in keff_dict.values()])
    keff_errors = np.array([v[3] for v in keff_dict.values()])

    keff_upper = (np.array([v[2][1] for v in keff_dict.values()])) * tau_tl_list
    keff_lower = (np.array([v[2][0] for v in keff_dict.values()])) * tau_tl_list

    y = keff_values * tau_tl_list
    y_err = keff_errors * tau_tl_list

    popt, pcov = curve_fit(linear_func, tau_tl_list, y, sigma=y_err, absolute_sigma=True)
    slope, intercept = popt
    koff_se = np.sqrt(np.diag(pcov))[0]
    cb_std_err = np.sqrt(np.diag(pcov))[1]

    k_off = slope
    c_b = intercept
    k_b = c_b / tau_int

    kb_se = cb_std_err/tau_int

    ci_list = compute_all_CI(popt, pcov, len(tau_tl_list))

    ci_list[1] = (np.array(ci_list[1]))/tau_int

    rss,_ = compute_rss_bic(linear_func, tau_tl_list, y, popt)

    t = np.linspace(0, max(tau_tl_list), 200)
    fit = linear_func(t, slope, intercept)

    fig, ax = plt.subplots()
    k_off_annot = rf"$k_{{\mathrm{{off}}}} = {k_off:.3f} \mathrm{{s^{{-1}}}}$"


    lower_errors = y - keff_lower  
    upper_errors = keff_upper - y  

    y_err1 = [lower_errors, upper_errors]

    data_handle = ax.errorbar(tau_tl_list, y, yerr=y_err1, fmt='o', color='black', capsize=3, elinewidth=1, label='data', zorder=2)
    fit_handle, = ax.plot(t, fit, color='grey', label=f'fit: {k_off_annot}', zorder=1)

    ax.set_xlabel(r"$\tau_{\mathrm{tl}}$ (seconds)")
    ax.set_ylabel(r"$k_{\mathrm{eff}} \, \tau_{\mathrm{tl}}$")
    current_ylim = ax.get_ylim()
    ax.set_ylim(bottom=0, top=current_ylim[1])
    ax.set_axisbelow(True)
    ax.grid(True, zorder=0)

    ax.legend(handles=[data_handle, fit_handle])

    return (k_off, k_b), (koff_se, kb_se), (ci_list[0], ci_list[1], rss), fig


def main_sequentialFit(input_dict, tau_int, sample_name, plot_semilog = True, output95CI=False, root_directory=None, destination_directory=None):
    """
    Residence time analysis using SPT data from timelapse experiments.
    Individually fits keff values from each timelapse experiment data and then computes photobleaching and dissociation rate.
    Args:
    - input_dict (dict): Dictionary where keys represent tau_tl (unit: seconds) and values represent folder path to csv SPT files or counts data. 
    - tau_int (float): camera integration time/exposure time in seconds.
    - sample_name: name of sample/condition
    - plot_semilog (bool=True, optional): whether to plot the hist fit in log-log scale.
    - balance_tl_weigths (bool=True, optional): whether to assign equal weight to each tau_tl experiment in the fitting.
    - output95CI (bool=False, optional): to output 95% confidence interval of fitted params instead of sd.
    - root_directory
    - destination_directory (str, optional): path to custom destination 

    Outputs: 
    - exponential_fits plot
    - linear fit plot
    - txt of results and rates.
    """
    first_val = next(iter(input_dict.values()))

    if isinstance(first_val, str):
        counts_dict = extract_counts(input_dict)
    elif (
        isinstance(first_val, tuple) and 
        len(first_val) == 3 and 
        isinstance(first_val[0], (list, np.ndarray))
    ):
        counts_dict = input_dict
    else:
        raise ValueError("Unknown input_dict format")
    
    # defining destinaiton dir 
    if destination_directory:
        destination_dir = os.path.join(destination_directory, f'{sample_name}_residenceTimeAnalysis')
        os.makedirs(destination_dir, exist_ok=True)
    
    if root_directory:
        destination_dir = os.path.join(root_directory, 'results', f'{sample_name}_residenceTimeAnalysis')
        os.makedirs(destination_dir, exist_ok=True)

    keff_dict, expFit_fig = compute_keffs(counts_dict, plot_semilog)
    rates, (koff_se, kb_se), linFit_metrics, linFit_fig = compute_koff(keff_dict, tau_int)
    
    if destination_directory or root_directory:
    # saving figures
        exp_fit_destination = os.path.join(destination_dir, f'singleExp_keff_histFit.pdf' )
        expFit_fig.savefig(exp_fit_destination)

        lin_fit_destination = os.path.join(destination_dir, f'singleExp_koff_kb_linFit.pdf')
        linFit_fig.savefig(lin_fit_destination)

    # txt to save/store results.
    txt_out = f"=== Residence Time Analysis of {sample_name} - Results ===\n\n"
    txt_out += "Inputs per timelapse experiment:\n"
    for tau_tl, (counts, total_counts, numFOVs) in counts_dict.items():
        txt_out += f"-  timelapse {tau_tl}s : numFOVs = {numFOVs} | total_numTracks = {total_counts}\n"
    
    txt_out += "\nk_eff values from fitting each tau_tl distribution to a single-exp decay:\n"
    A_values = []
    for tau_tl, (A_fit, keff_fit, keff_ci, keff_se) in keff_dict.items():
        if output95CI:
            keff_hw_ci = compute_half_width_ci(keff_ci)
            txt_out += f"-  timelapse {tau_tl}s : k_eff = {keff_fit:.4f} ± {keff_hw_ci:.4f} s⁻¹\n"
        else: 
            txt_out += f"-  timelapse {tau_tl}s : k_eff = {keff_fit:.4f} ± {keff_se:.4f} s⁻¹\n"
        A_values.append(A_fit)
    
    txt_out += f"\nCamera integration (exposure time): {tau_int}s\n"

    k_off_ci = compute_half_width_ci(linFit_metrics[0])
    k_b_ci = compute_half_width_ci(linFit_metrics[1])

    
    txt_out += "\nk_off and k_b values from linear reg of k_eff values:\n"
    if output95CI:
        txt_out += f"-  Dissociation rate (k_off): {rates[0]:.4f} ± {k_off_ci:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {(1/rates[0]):.2f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {rates[1]:.4f} ± {k_b_ci:.4f} s⁻¹\n"
    else:
        txt_out += f"-  Dissociation rate (k_off): {rates[0]:.4f} ± {koff_se:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {(1/rates[0]):.2f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {rates[1]:.4f} ± {kb_se:.4f} s⁻¹\n"
    txt_out += f"-  Residual Sum of Squares of linear fit = {linFit_metrics[2]:.4f}\n"

    if output95CI:
        txt_out += "\nNote: '±' denotes the half-width of the 95% confidence interval around the fitted parameter.\n"
    else: 
        txt_out += "\nNote: '±' denotes the s.d. of the fitted parameter.\n"
    print(txt_out)

    # print(f"A_values = {', '.join(str(round(A_value)) for A_value in A_values)}")
    print(f'\n')

    if root_directory or destination_directory:
        txt_destination = os.path.join(destination_dir, 'analysisResults_singleExp_sequentialFit.txt')
        with open(txt_destination, 'w', encoding='utf-8') as file:
            file.write(txt_out)



def get_initial_params(counts_dict, tau_int):
    """
    Input:
    - counts_dict
    - tau_int

    Returns:
    - list of A params guess in order of tl
    - koff guess
    - kb guess
    """
    A_guess_list = []
    keff_values_list = []
    for tau_tl in sorted(counts_dict.keys()):
        value = counts_dict[tau_tl]
        counts = value[0]

        if isinstance(counts, list):
            counts = np.array(counts, dtype=np.int64)  

        actual_x = np.arange(2, len(counts)+1.01, 1) * tau_tl
        fit_x = np.linspace(min(actual_x), max(actual_x), 200)
        
        # guessing initial A and k_eff using log transform fit
        valid = counts > 0
        actual_x_valid = actual_x[valid]
        counts_valid = counts[valid]
        log_counts = np.log(counts_valid)
        slope, intercept = np.polyfit(actual_x_valid, log_counts, 1)
        k_eff_guess = -slope
        A_guess = np.exp(intercept)

        A_guess_list.append(A_guess)
        keff_values_list.append(k_eff_guess)

    tau_tl_sorted = np.array(sorted(counts_dict.keys()))
    keff_values_list = np.array(keff_values_list)

    x = tau_tl_sorted
    y = tau_tl_sorted * keff_values_list

    slope, intercept, _, _, _ = linregress(x, y)
     
    koff_guess = slope
    kb_guess = intercept / tau_int

    return A_guess_list, koff_guess, kb_guess

def get_singleExp_params(counts_dict, tau_int):
    tau_tl_values = sorted(counts_dict.keys())

    t_concat, counts_concat, tau_tl_concat = [], [], []

    for tau_tl in tau_tl_values:
        counts_array, _, _ = counts_dict[tau_tl] # total_counts is unused
        if isinstance(counts_array, list):
            counts_array = np.array(counts_array, dtype=np.int64)
        t_vals = (np.arange(len(counts_array)) + 2) * tau_tl
        t_concat.append(t_vals)
        counts_concat.append(counts_array)
        tau_tl_concat.append(np.full_like(t_vals, tau_tl))

    t_concat = np.concatenate(t_concat)
    counts_concat = np.concatenate(counts_concat)
    tau_tl_concat = np.concatenate(tau_tl_concat)
    
    # model
    def single_exp_decay(x, *params):
        t, tau_tl = x
        n = len(tau_tl_values)
        A_coeffs = params[:n]
        k_off, k_b = params[-2], params[-1]
        y = np.empty_like(t, dtype=float)
        for i, tau in enumerate(tau_tl_values):
            mask = tau_tl == tau
            y[mask] = A_coeffs[i] * np.exp(- (k_b * (tau_int / tau) + k_off) * t[mask])
        return y
    
    
    # curve_fit
    initial_A_guesses, koff_guess, kb_guess = get_initial_params(counts_dict, tau_int)
    
    koff_guess = max(koff_guess, 1e-6)
    kb_guess = max(kb_guess, 1e-6)
    initial_A_guesses = [max(A, 1e-6) for A in initial_A_guesses]

    p0_single = initial_A_guesses + [koff_guess, kb_guess]
    xdata = np.vstack((t_concat, tau_tl_concat))
    ydata = counts_concat

    bounds_single = ([0.0] * len(tau_tl_values) + [0.0, 0.0],
                     [np.inf] * len(tau_tl_values) + [np.inf, np.inf])

    
    popt_single, pcov_single = curve_fit(single_exp_decay, xdata, ydata, p0=p0_single, maxfev=10000, bounds=bounds_single)
    
    A_fits_single = popt_single[:len(tau_tl_values)]
    koff_fit, kb_fit = popt_single[-2], popt_single[-1]

    return A_fits_single, koff_fit, kb_fit




def global_singleExp(counts_dict, tau_int, balance_tl_weights, plot_semilog, min_residence_time, cap_residence_time):
    
    tau_tl_values = sorted(counts_dict.keys())
    t_concat, counts_concat, tau_tl_concat, total_counts_concat = [], [], [], []

    for i, tau_tl in enumerate(tau_tl_values):
        counts_array, total_counts, _ = counts_dict[tau_tl]
        if isinstance(counts_array, list):
            counts_array = np.array(counts_array, dtype=np.int64)
        t_vals = (np.arange(len(counts_array)) + 1) * tau_tl # first index in the counts array is at time = tau_tl and not time = 0
        t_concat.append(t_vals)
        counts_concat.append(counts_array)
        tau_tl_concat.append(np.full_like(t_vals, tau_tl))
        total_counts_concat.append(np.full_like(t_vals, total_counts))
        
    t_concat = np.concatenate(t_concat)
    counts_concat = np.concatenate(counts_concat)
    tau_tl_concat = np.concatenate(tau_tl_concat)

    xdata = np.vstack((t_concat, tau_tl_concat))
    ydata = counts_concat

    # model
    def single_exp_decay(x, *params):
        t, tau_tl = x
        n = len(tau_tl_values)
        A_coeffs = params[:n]
        k_off, k_b = params[-2], params[-1]
        y = np.empty_like(t, dtype=float)
        for i, tau in enumerate(tau_tl_values):
            mask = tau_tl == tau # creating a boolean array of same length as tau_tl with True if the value in tau_tl = tau else False.
            y[mask] = A_coeffs[i] * np.exp(- (k_b * (tau_int / tau) + k_off) * t[mask])
        return y
    

    A_guesses,koff_guess,kb_guess = get_singleExp_params(counts_dict, tau_int)

    if min_residence_time == 0:
        koff_upper = np.inf
    else:
        koff_upper = 1/min_residence_time

    if cap_residence_time == np.inf:
        koff_lower = 0.0
    else: 
        koff_lower = 1/cap_residence_time

    p0_single = list(A_guesses) + [koff_guess, kb_guess]
    bounds_single = ([0.0] * len(tau_tl_values) + [koff_lower, 0.0],
                     [np.inf] * len(tau_tl_values) + [koff_upper, np.inf])

    if balance_tl_weights:
        # Equalise the influence of each tau_tl group
        weights = []
        for tau_tl in tau_tl_values:
            num_points = sum(tau_tl_concat == tau_tl)
            weight = np.sqrt(num_points)  # curve_fit minimises sum((y - f(x)) / sigma)**2
            weights.append(weight)

        tau_to_sigma = {tau: w for tau, w in zip(tau_tl_values, weights)}
        sigma = np.array([tau_to_sigma[tau] for tau in tau_tl_concat], dtype=float)
        popt_single, pcov_single = curve_fit(single_exp_decay, xdata, ydata, p0=p0_single,maxfev=10000,
                                             bounds=bounds_single, sigma=sigma, absolute_sigma=False )
    
    else: 
        popt_single, pcov_single = curve_fit(single_exp_decay, xdata, ydata, p0=p0_single, maxfev=10000, bounds=bounds_single)

    A_fits = popt_single[:len(tau_tl_values)]
    koff_fit, kb_fit = popt_single[-2:]
    koff_se, kb_se = np.sqrt(np.diag(pcov_single))[-2:]

    # metrics - 95% CI + BIC
    rss, bic = compute_rss_bic(single_exp_decay, xdata, ydata, popt_single)
    koff_ci, kb_ci = compute_all_CI(popt_single, pcov_single, len(ydata))[-2:]

    # hist fit plot
    fig, ax = plt.subplots()
    for i, tau_tl in enumerate(tau_tl_values):
        counts, total_counts, _ = counts_dict[tau_tl]
        counts = np.array(counts)
        perc_surv = counts / total_counts
        idx_trunc = find_first_valid_index(counts)
        perc_surv_trunc = perc_surv[:idx_trunc]

        actual_x = (np.arange(len(perc_surv_trunc)) + 1) * tau_tl
        fit_x = np.linspace(actual_x.min(), actual_x.max(), 200)
        rate = koff_fit + kb_fit * (tau_int / tau_tl)
        y_fit = A_fits[i] * np.exp(-rate * fit_x) / total_counts

        ax.scatter(actual_x, perc_surv_trunc, label=f'{tau_tl}s data', zorder=2)
        ax.plot(fit_x, y_fit, label=f'{tau_tl}s fit', zorder=1)
    
    ax.set_xscale('log')
    if plot_semilog:
        ax.set_yscale('log')
        ax.set_xscale('linear')
    
    ax.set_xlabel(r"n$\tau_{\mathrm{tl}}$ (seconds)")
    ax.set_ylabel("fraction surviving")

    ax.set_axisbelow(True)
    ax.grid(True, which='both', linestyle='--', linewidth=0.5, zorder=0)
    ax.legend()
    ax.set_title("single-exp global fit")
    fig.tight_layout()

    # lin plot
    fig2, ax2 = plt.subplots()
    scatter_vals = [(koff_fit + kb_fit * (tau_int / tau)) * tau for tau in tau_tl_values]
    ax2.scatter(tau_tl_values, scatter_vals, color='black', label='data', zorder=2)
    fit_x = np.linspace(min(tau_tl_values), max(tau_tl_values), 100)
    fit_y = koff_fit * fit_x + kb_fit * tau_int
    ax2.plot(fit_x, fit_y, linestyle='--', color='grey', label=f'fit: $k_{{off}}={koff_fit:.3f}$', zorder = 1)
    ax2.set_xlabel(r"$\tau_{\mathrm{tl}}$ (seconds)")
    ax2.set_ylabel(r"$k_{\mathrm{eff}} \, \tau_{\mathrm{tl}}$")
    ax2.set_axisbelow(True)
    ax2.grid(True, zorder=0)
    ax2.legend()
    fig2.tight_layout()
    
    return (
        (A_fits, koff_fit, kb_fit),
        (koff_se, kb_se),
        (koff_ci, kb_ci),
        (rss, bic),
        (fig, fig2)
    )

def compute_keffs_perCell_single(tracks_dict, kb_value, tau_int):
    """
    Args:
    - tracks_dict: keys represent tau_tl and values represent path to folder containing tracks files for that tau_tl.
    - kb_value: tuple of (kb_value, kb_std_error) from global fit
    - tau_int: integration time

    Returns:
    - results_dict: keys represent tau_tl
        - value: 
            - first element: figure/plot of the per cell histogram fits 
            - second element: list of figures of each cell scatter and fit.
            - third element: list of (keff, keff_err) values 
            - fourth element: list of (koff, koff_err) values
            - fifth element: list of total_counts
    """
    
    def singleExp_decay(t, A, k_eff):
        return A*np.exp(-(k_eff)*t)
    
    results_dict = {}

    # global_xtau = []
    # global_keffxtau = []
    all_tau_tl = []
    all_koffs = []
    all_keffs = []

    for tau_tl, tracks_dir in tracks_dict.items():
        
        koff_values = []
        keff_values = []
        total_counts = []
        indiv_fig_list = []

        fig, ax = plt.subplots()

        for filename in os.listdir(tracks_dir):
            if not filename.endswith('.csv'):
                continue
            file_path = os.path.join(tracks_dir, filename)
            df = pd.read_csv(file_path, comment = '#')
            list_lengths = df.iloc[:,1]
            bins = np.arange(1.5, max(list_lengths) + 0.6, 1)

            counts,_ = np.histogram(list_lengths, bins=bins)
            counts = np.array(counts)
            total_count = np.sum(counts)

            x_data = np.arange(tau_tl, tau_tl*len(counts)+0.01, tau_tl)
            x_fit = np.linspace(min(x_data), max(x_data), 200)

            valid = counts >0
            x_data_valid = x_data[valid]
            perc_valid = counts[valid]
            log_perc = np.log(perc_valid)

            slope, intercept = np.polyfit(x_data_valid, log_perc, 1)
            k_eff_guess = -slope
            A_guess = np.exp(intercept)
            p0 = [A_guess, k_eff_guess]

            
            popt, pcov = curve_fit(singleExp_decay, x_data, counts, p0=p0)
            A_fit, k_eff_fit = popt
            perr = np.sqrt(np.diag(pcov))

            A_err, k_eff_err = perr
            _,ci = compute_all_CI(popt, pcov, len(counts))

            koff_val = k_eff_fit - (tau_int/tau_tl)*kb_value[0]
            koff_err = np.sqrt(k_eff_err**2 +((tau_int/tau_tl)*kb_value[1])**2)

            # koff_ci = (ci[0] + kb_value[1][0], ci[1] + kb_value[1][1])
            k_off_annot = rf"$k_{{\mathrm{{off}}}} = {koff_val:.3f} \mathrm{{s^{{-1}}}}$"

            perc_surviving = counts / total_count

            # scatter = ax.scatter(x_data, perc_surviving, zorder = 2)
            y_fit = singleExp_decay(x_fit, A_fit, k_eff_fit) / total_count

            plot = ax.plot(x_fit, y_fit, zorder = 1)

            # Create individual figure for each cell
            fig_indiv, ax_indiv = plt.subplots()
            ax_indiv.scatter(x_data, perc_surviving, label="data", zorder=2)
            ax_indiv.plot(x_fit, y_fit, label=f"fit: {k_off_annot}", zorder=1)
            ax_indiv.set_yscale('log')
            ax_indiv.set_axisbelow(True)
            ax_indiv.grid(True, linestyle='--', linewidth=0.5, zorder=0)
            ax_indiv.set_xlabel("time (seconds)")
            ax_indiv.set_ylabel("fraction surviving")
            ax_indiv.set_title(f"Cell={len(koff_values)+1}, interval_time={tau_tl}s, numtracks = {total_count}")
            ax_indiv.legend()

            keff_value = (k_eff_fit, k_eff_err)
            keff_values.append(keff_value)

            koff_value = (koff_val, koff_err)
            koff_values.append(koff_value)

            total_counts.append(total_count)
            indiv_fig_list.append(fig_indiv)
        
        ax.set_axisbelow(True)
        ax.grid(True, zorder=0)

        # ax.set_xscale('log')
        ax.set_xlabel(r"time (seconds)")
        ax.set_ylabel("fraction surviving")
        ax.set_title(f'{tau_tl}s interval - single-exp keff fits per cell')
        
        # global_xtau.extend([tau_tl] * len(keff_values))
        # global_keffxtau.extend([k_eff * tau_tl for k_eff, _ in keff_values])
        all_tau_tl.extend([tau_tl] * len(koff_values))
        all_koffs.extend([k_off for k_off, _ in koff_values])
        all_keffs.extend([keff for keff,_ in keff_values])

        
        results_dict[tau_tl] = (fig, indiv_fig_list, keff_values, koff_values, total_counts)


    unique_tau_tl = sorted(set(all_tau_tl))
    tau_to_pos = {tau: i for i, tau in enumerate(unique_tau_tl)}
    x_positions = [tau_to_pos[tau] for tau in all_tau_tl]

    fig2, ax_summary = plt.subplots(figsize=(4, 3))
    ax_summary.scatter(x_positions, all_keffs, alpha=0.6)
    ax_summary.set_xticks(range(len(unique_tau_tl)))
    ax_summary.set_xticklabels([str(tau) for tau in unique_tau_tl], rotation=45)
    ax_summary.set_xlabel('tau_tl (seconds)')
    ax_summary.set_ylabel('keff')

    left_pad = -0.5
    right_pad = len(unique_tau_tl) - 0.5
    ax_summary.set_xlim(left_pad, right_pad)

    return results_dict, fig2


def global_doubleExp(counts_dict, tau_int, amplitude_multiplier, balance_tl_weights, plot_semilog, cap_B_value, min_residence_time, cap_residence_time):

    tau_tl_values = sorted(counts_dict.keys())
    t_concat, counts_concat, tau_tl_concat, total_counts_concat = [], [], [], []

    for i, tau_tl in enumerate(tau_tl_values):
        counts_array, total_counts, _ = counts_dict[tau_tl]
        if isinstance(counts_array, list):
            counts_array = np.array(counts_array, dtype=np.int64)
        t_vals = (np.arange(len(counts_array)) + 1) * tau_tl
        t_concat.append(t_vals)
        counts_concat.append(counts_array)
        tau_tl_concat.append(np.full_like(t_vals, tau_tl))
        total_counts_concat.append(np.full_like(t_vals, total_counts))

    t_concat = np.concatenate(t_concat)
    counts_concat = np.concatenate(counts_concat)
    tau_tl_concat = np.concatenate(tau_tl_concat)

    xdata = np.vstack((t_concat, tau_tl_concat))
    ydata = counts_concat

    def double_exp_decay(x, *params):
        t, tau_tl  = x
        n = len(tau_tl_values)
        A_coeffs = params[:n]
        B, k_off1, k_off2, k_b = params[-4:]
        y = np.empty_like(t, dtype=float)
        for i, tau in enumerate(tau_tl_values):
            mask = tau_tl == tau
            rate1 = (k_b * (tau_int / tau) + k_off1)
            rate2 = (k_b * (tau_int / tau) + k_off2)
            y[mask] = A_coeffs[i] * (B * np.exp(-rate1 * t[mask]) + (1 - B) * np.exp(-rate2 * t[mask]))
        return y

    # curve_fit
    A_guesses,koff_guess,kb_guess = get_singleExp_params(counts_dict, tau_int)

    p0_double = A_guesses.tolist() + [0.5, koff_guess, koff_guess*10, kb_guess]
    upper_bounds = (A_guesses*amplitude_multiplier).tolist()
    
    if min_residence_time == 0:
        koff_upper = np.inf
    else:
        koff_upper = 1/min_residence_time

    if cap_residence_time == np.inf:
        koff_lower = 0.0
    else: 
        koff_lower = 1/cap_residence_time
    
    bounds_double = ([0.0] * len(tau_tl_values) + [(1-cap_B_value), koff_lower, koff_lower, 0.0],
                     upper_bounds + [cap_B_value, koff_upper, koff_upper, np.inf])

    if balance_tl_weights:
        # Equalise the influence of each tau_tl group
        weights = []
        for tau_tl in tau_tl_values:
            num_points = sum(tau_tl_concat == tau_tl)
            weights.append(np.sqrt(num_points))
        tau_to_sigma = {tau: w for tau, w in zip(tau_tl_values, weights)}
        sigma = np.array([tau_to_sigma[tau] for tau in tau_tl_concat], dtype=float)

        popt_double, pcov_double = curve_fit(
            double_exp_decay, xdata, ydata, p0=p0_double,
            maxfev=20000, bounds=bounds_double,
            sigma=sigma, absolute_sigma=False
        )
    else:
        popt_double, pcov_double = curve_fit(
            double_exp_decay, xdata, ydata, p0=p0_double,
            maxfev=20000, bounds=bounds_double
        )

    A_fits = popt_double[:len(tau_tl_values)]
    B_fit, koff1_fit, koff2_fit, kb_fit = popt_double[-4:]
    B_se, koff1_se, koff2_se, kb_se = np.sqrt(np.diag(pcov_double)[-4:])

    if koff1_fit > koff2_fit:
        koff1_fit, koff2_fit = koff2_fit, koff1_fit
        koff1_se, koff2_se = koff2_se, koff1_se 
        B_fit = 1 - B_fit


    # metrics - 95% CI + BIC
    rss, bic = compute_rss_bic(double_exp_decay, xdata, ydata, popt_double)
    B_ci, koff1_ci, koff2_ci, kb_ci = compute_all_CI(popt_double, pcov_double, len(ydata))[-4:]

    # hist fit 
    fig3, ax3 = plt.subplots()
    for i, tau_tl in enumerate(tau_tl_values):
        counts, total_counts, _ = counts_dict[tau_tl]
        counts = np.array(counts)
        perc_surv = counts / total_counts
        idx_trunc = find_first_valid_index(counts)
        perc_surv_trunc = perc_surv[:idx_trunc]
        actual_x = (np.arange(len(perc_surv_trunc)) + 1) * tau_tl
        fit_x = np.linspace(actual_x.min(), actual_x.max(), 200)
        rate1 = kb_fit * (tau_int / tau_tl) + koff1_fit
        rate2 = kb_fit * (tau_int / tau_tl) + koff2_fit
        y_fit = A_fits[i] * (B_fit * np.exp(-rate1 * fit_x) + (1 - B_fit) * np.exp(-rate2 * fit_x)) / total_counts

        ax3.scatter(actual_x, perc_surv_trunc, label=f'{tau_tl}s data', zorder=2)
        ax3.plot(fit_x, y_fit, label=f'{tau_tl}s fit', zorder=1)
    
    ax3.set_xscale('log')
    if plot_semilog:
        ax3.set_yscale('log')
        ax3.set_xscale('linear')
    ax3.set_xlabel(r"n$\tau_{\mathrm{tl}}$ (seconds)")
    ax3.set_ylabel("fraction surviving")
    ax3.set_axisbelow(True)
    ax3.grid(True, which='both', linestyle='--', linewidth=0.5, zorder=0)
    ax3.legend()
    ax3.set_title("double-exp global fit")
    fig3.tight_layout()

    # lin fit
    fig4, ax4 = plt.subplots()
    fit_x = np.linspace(min(tau_tl_values), max(tau_tl_values), 100)

    legend_labels = [f'long-lived ({B_fit*100:.2f}%)', f'short-lived ({(1 - B_fit)*100:.2f}%)']
    colour_scatter = ['#4B5D67', '#7A6651']
    colour_scatter = ['#2F4F6F', '#80633E']  
    colour_fit = ['#9BB8C8', '#C2B08B']      

    all_handles, all_labels = [], []

    for i, k_off in enumerate([koff1_fit, koff2_fit]):
        scatter_vals_d = [(k_off + kb_fit * (tau_int / tau)) * tau for tau in tau_tl_values]
        
        scatter = ax4.scatter(tau_tl_values, scatter_vals_d, alpha=0.7, zorder = 2, color = colour_scatter[i])
        fit_y_d = k_off * fit_x + kb_fit * tau_int
        fit, = ax4.plot(fit_x, fit_y_d, linestyle='--', zorder = 1, color = colour_fit[i])

        header = Line2D([], [], color='none', marker=None, linewidth=0)

        all_handles.extend([header, scatter, fit])
        all_labels.extend([legend_labels[i], '   data', f'   fit: $k_{{off}}={k_off:.3f}$'])
        if i == 0:
            spacer = Line2D([], [], color='none', marker=None, linewidth=0)
            all_handles.append(spacer)
            all_labels.append("")

    ax4.set_xlabel(r"$\tau_{\mathrm{tl}}$ (seconds)")
    ax4.set_ylabel(r"$k_{\mathrm{eff}} \, \tau_{\mathrm{tl}}$")
    ax4.set_axisbelow(True)
    ax4.grid(True, zorder=0)
    legend = ax4.legend(handles=all_handles, labels=all_labels, labelspacing=0.4)

    for i, label in enumerate(all_labels):
        if label in legend_labels:
            legend.get_texts()[i].set_fontweight('bold')
    fig4.tight_layout()

    return (         
    (A_fits, B_fit, koff1_fit, koff2_fit, kb_fit),
    (B_se, koff1_se, koff2_se, kb_se),
    (B_ci, koff1_ci, koff2_ci, kb_ci),
    (rss, bic),
    (fig3, fig4)
    )

def compute_keffs_perCell_double(tracks_dict, koff1, koff2, kb, tau_int):
    """
    Args:
    - tracks_dict: keys represent tau_tl and values represent path to folder containing tracks files for that tau_tl.

    Returns:
    - results_dict: keys represent tau_tl
        - value: 
            - first element: figure/plot of the per cell histogram fits 
            - second element: list of individual cell figures.
            - third element: list of (keff, keff_err) values 
            - fourth element: list of total_counts
                - e.g. {0.5: (figure, [(keff1, keff_err1), (keff2, keff_err2)], [counts_1, counts_2])}
    - matplotlib figure: distribution of B values across all tau_tl   
    """
    if koff1 > koff2:
        koff1, koff2 = koff2, koff1
    
    def model(tau_tl):
        keff1 = kb*(tau_int / tau_tl) + koff1
        keff2 = kb*(tau_int / tau_tl) + koff2
        def doubleExp_decay(t, A, B):
            return A*(B*np.exp(-(keff1)*t) + (1-B)*np.exp(-(keff2)*t))
        return doubleExp_decay

    results_dict = {}
    all_tau_tl = []
    all_Bs = []

    for tau_tl, tracks_dir in tracks_dict.items():
        
        doubleExp_decay1 = model(tau_tl)

        B_values = []
        total_counts = []
        indiv_fig_list = []

        fig, ax = plt.subplots()

        for filename in os.listdir(tracks_dir):
            if not filename.endswith('.csv'):
                continue
            file_path = os.path.join(tracks_dir, filename)
            df = pd.read_csv(file_path, comment = '#')
            list_lengths = df.iloc[:,1]
            bins = np.arange(1.5, max(list_lengths) + 0.6, 1)

            counts,_ = np.histogram(list_lengths, bins=bins)
            counts = np.array(counts)
            total_count = np.sum(counts)

            x_data = np.arange(tau_tl, tau_tl*len(counts)+0.01, tau_tl)
            x_fit = np.linspace(min(x_data), max(x_data), 200)

            p0 = [1, 0.5]
            
            popt, pcov = curve_fit(
                doubleExp_decay1,
                x_data,
                counts,
                p0=p0,
                bounds=([0, 0.0], [np.inf, 1.0]),
                maxfev=10000  # also add this to avoid convergence issues
                )
            A_fit, B_fit = popt
            perr = np.sqrt(np.diag(pcov))
            A_err, B_err = perr
            _,ci = compute_all_CI(popt, pcov, len(counts))

            B_annot = f"B = {(B_fit*100):.1f}"

            perc_surviving = counts / total_count
            # scatter = ax.scatter(x_data, perc_surviving, zorder = 2)
            y_fit = doubleExp_decay1(x_fit, A_fit, B_fit) / total_count
            plot = ax.plot(x_fit, y_fit, zorder = 1)

            fig_indiv, ax_indiv = plt.subplots()
            ax_indiv.scatter(x_data, perc_surviving, label="data", zorder=2)
            ax_indiv.plot(x_fit, y_fit, label=f"fit: {B_annot}", zorder=1)
            ax_indiv.set_yscale('log')
            ax_indiv.set_axisbelow(True)
            ax_indiv.grid(True, linestyle='--', linewidth=0.5, zorder=0)
            ax_indiv.set_xlabel("time (seconds)")
            ax_indiv.set_ylabel("fraction surviving")
            ax_indiv.set_title(f"Cell={len(B_values)+1}, interval_time={tau_tl}s, numtracks = {total_count}")
            ax_indiv.legend()
            
            B_value = (B_fit, B_err)
            B_values.append(B_value)
            total_counts.append(total_count)
            indiv_fig_list.append(fig_indiv)
        
        ax.set_axisbelow(True)
        ax.grid(True, zorder=0)
        ax.set_xscale('log')
        ax.set_xlabel(r"time (seconds)")
        ax.set_ylabel("fraction surviving")
        ax.set_title(f'{tau_tl} - double-exp fits per cell')
        
        all_tau_tl.extend([tau_tl] * len(B_values))
        all_Bs.extend([B_val for B_val, _ in B_values])

        results_dict[tau_tl] = (fig, indiv_fig_list, B_values, total_counts)
    
    unique_tau_tl = sorted(set(all_tau_tl))
    tau_to_pos = {tau: i for i, tau in enumerate(unique_tau_tl)}
    x_positions = [tau_to_pos[tau] for tau in all_tau_tl]

    fig2, ax_summary = plt.subplots(figsize=(4, 3))
    ax_summary.boxplot(all_Bs, positions=[1], widths=0.4,
                   medianprops=dict(color='black'))

    ax_summary.scatter(np.random.normal(1, 0.05, size=len(all_Bs)), all_Bs, alpha=0.6, s=10)

    ax_summary.set_xlim(0.5, 1.5)
    ax_summary.set_ylabel('fraction of long-lived population (B)')
    ax_summary.grid(True, axis='y', linestyle='--', alpha=0.5)
    ax_summary.set_xlabel('')          
    ax_summary.set_xticks([])



    return results_dict, fig2




def main_globalFit(input_dict, tau_int, sample_name, amplitude_multiplier=1000, plot_semilog=True, balance_tl_weights=True, min_residence_time=0.0, cap_residence_time=np.inf, cap_B_value=0.999, output95CI=False, root_directory = None, destination_directory=None):
    """
    Residence time analysis using SPT data from timelapse experiments.
    Performs a global single and double exponential fit to estimate kinetics parameters directly.

    Args:
    - input_dict (dict): tracks_dict or counts_dict
    - tau_int (float): camera integration time (s)
    - sample_name (str)
    - amplitude_multiplier (float): defines the upper bound of the double exponential amplitudes relative to the single exp fitted amplitudes.
    - balance_tl_weights (boolean): to equalise the influence of each tau_tl group in the fit.
    - root_directory (str, optional)
    - destination_directory (str, optional)
    - plot_semilog (bool=True, optional): whether to plot the hist fit in log-log scale.
    - balance_tl_weigths (bool=True, optional): whether to assign equal weight to each tau_tl experiment in the fitting
    - min_residence_time (float=0.0, optional): to set a minimum residence time
    - cap_residence_time (float=np.inf, optional): to cap the residence time
    - cap_B_value (float=1.0, optional): to cap the fraction of either population in double exp fit.
    - output95CI (bool=False, optional): to output 95% confidence interval of fitted params instead of sd.

    Outputs:
    - PDF plots
    - TXT summary of results
    """
    first_val = next(iter(input_dict.values()))

    if isinstance(first_val, str):
        counts_dict = extract_counts(input_dict)
        compute_single_cell_fits = True
    elif (
        isinstance(first_val, tuple) and 
        len(first_val) == 3 and 
        isinstance(first_val[0], (list, np.ndarray))
    ):
        counts_dict = input_dict
        compute_single_cell_fits = False
    else:
        raise ValueError("Unknown input_dict format")

    (A_fits_s, koff_fit_s, kb_fit_s), (koff_se, kb_se), (koff_ci_s, kb_ci_s), (rss_s, bic_s),(fig, fig2) = global_singleExp(counts_dict, tau_int, balance_tl_weights, plot_semilog, min_residence_time, cap_residence_time)
    (A_fits_d, B_fit_d, koff1_fit_d, koff2_fit_d, kb_fit_d), (B_se, koff1_se, koff2_se, kb_se_d), (B_ci_d, koff1_ci_d, koff2_ci_d, kb_ci_d), (rss_d, bic_d), (fig3, fig4) = global_doubleExp(counts_dict, tau_int, amplitude_multiplier, balance_tl_weights, plot_semilog, cap_B_value, min_residence_time, cap_residence_time)

    # save figures
    if destination_directory:
        destination_dir = os.path.join(destination_directory, f'{sample_name}_residenceTimeAnalysis')
        os.makedirs(destination_dir, exist_ok=True)
    
    if root_directory:
        destination_dir = os.path.join(root_directory, 'results', f'{sample_name}_residenceTimeAnalysis')
        os.makedirs(destination_dir, exist_ok=True)

    if destination_directory or root_directory:
        singleExp_histfit_path = os.path.join(destination_dir, 'singleExp_global_histFit.pdf')
        fig.savefig(singleExp_histfit_path)

        singleExp_linFit_path = os.path.join(destination_dir, 'singleExp_global_linPlot.pdf')
        # fig2.savefig(singleExp_linFit_path)

        doubleExp_histfit_path = os.path.join(destination_dir, 'doubleExp_global_histFit.pdf')
        fig3.savefig(doubleExp_histfit_path)

        doubleExp_linFit_path = os.path.join(destination_dir, 'doubleExp_global_linPlot.pdf')
        # fig4.savefig(doubleExp_linFit_path)


    def half_width_ci(ci):
        return (ci[1] - ci[0]) / 2

    koff_hw_s = half_width_ci(koff_ci_s)
    kb_hw_s = half_width_ci(kb_ci_s)

    B_hw_d = half_width_ci(B_ci_d)
    koff1_hw_d = half_width_ci(koff1_ci_d)
    koff2_hw_d = half_width_ci(koff2_ci_d)
    kb_hw_d = half_width_ci(kb_ci_d)

    # txt output
    txt_out = f"=== Residence Time Analysis of {sample_name} - Results ===\n\n"
    txt_out += "Inputs per timelapse experiment:\n"
    for tau_tl, (counts, total_counts, numFOVs) in counts_dict.items():
        txt_out += f"-  timelapse {tau_tl}s : numFOVs = {numFOVs} | total_numTracks = {total_counts}\n"

    txt_out += f"\nCamera integration (exposure time): {tau_int}s\n"

    # single exp
    txt_out += "\nResults from a global single-exp decay fit:\n"
    if output95CI:
        txt_out += f"-  Dissociation rate (k_off): {koff_fit_s:.4f} ± {koff_hw_s:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff_fit_s:.2f} ± {((koff_hw_s)/((koff_fit_s)**2)):.4f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {kb_fit_s:.4f} ± {kb_hw_s:.4f} s⁻¹\n"
    else:
        txt_out += f"-  Dissociation rate (k_off): {koff_fit_s:.4f} ± {koff_se:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff_fit_s:.2f} ± {((koff_se)/((koff_fit_s)**2)):.4f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {kb_fit_s:.4f} ± {kb_se:.4f} s⁻¹\n" 
    txt_out += f"-  Residual sum of squares: {rss_s:.4f}\n"
    txt_out += f"-  BIC value: {bic_s:.4f}\n"

    # double exp
    txt_out += "\nResults from a global double-exp decay fit:\n"
    if output95CI:
        txt_out += f"-  Fraction of long-lived population (B): {B_fit_d:.3f} ± {B_hw_d:.3f} (95% CI)\n"
        txt_out += f"-  Slow dissociation rate (k_off1): {koff1_fit_d:.4f} ± {koff1_hw_d:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff1_fit_d:.2f} ± {((koff1_hw_d)/((koff1_fit_d)**2)):.4f} s\n"
        txt_out += f"-  Fast dissociation rate (k_off2): {koff2_fit_d:.4f} ± {koff2_hw_d:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff2_fit_d:.2f} ± {((koff2_hw_d)/((koff2_fit_d)**2)):.4f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {kb_fit_d:.4f} ± {kb_hw_d:.4f} s⁻¹\n"
    else:
        txt_out += f"-  Fraction of long-lived population (B): {B_fit_d:.3f} ± {B_se:.3f} (95% CI)\n"
        txt_out += f"-  Slow dissociation rate (k_off1): {koff1_fit_d:.4f} ± {koff1_se:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff1_fit_d:.2f} ± {((koff1_se)/((koff1_fit_d)**2)):.4f} s\n"
        txt_out += f"-  Fast dissociation rate (k_off2): {koff2_fit_d:.4f} ± {koff2_se:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff2_fit_d:.2f} ± {((koff2_se)/((koff2_fit_d)**2)):.4f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {kb_fit_d:.4f} ± {kb_se_d:.4f} s⁻¹\n"
    txt_out += f"-  Residual sum of squares: {rss_d:.4f}\n"
    txt_out += f"-  BIC value: {bic_d:.4f}\n"
    
    if output95CI:
        txt_out += "\nNote: '±' denotes the half-width of the 95% confidence interval around the fitted parameter.\n"
    else:
        txt_out += "\nNote: '±' denotes the s.d. of the fitted parameter.\n"
        
    print(txt_out)

    # print(f"\nsingleExp A values = " + ", ".join([str(A_fits_s[0])] + [str(round(v)) for v in A_fits_s[1:]]))
    # print(f"\ndoubleExp A values = " + ", ".join([str(A_fits_d[0])] + [str(round(v)) for v in A_fits_d[1:]]))
    # print('\n')

    # save .txt
    if root_directory or destination_directory:
        txt_destination = os.path.join(destination_dir, 'analysisResults_globalFits.txt')
        with open(txt_destination, 'w', encoding = 'utf-8') as f:
            f.write(txt_out)

    # single cell analysis
    if compute_single_cell_fits:
        if bic_s < bic_d:
            kb_val = (kb_fit_s, kb_se)
            results_dict, figures2 = compute_keffs_perCell_single(input_dict, kb_val, tau_int)
            if destination_directory or root_directory:
                per_cell_dest = os.path.join(destination_dir, 'perCell - singleExp_keff_histFits')
                os.makedirs(per_cell_dest, exist_ok=True)
                fig_dest = os.path.join(per_cell_dest, f'keffs_perCell_plot.pdf')
                figures2.savefig(fig_dest)
            
            for tau_tl, (figures1,fig_list,_,_,_) in results_dict.items():
                if root_directory or destination_directory:
                    figures1_dest = os.path.join(destination_dir, 'perCell - singleExp_keff_histFits', f'{tau_tl}_combinedFits.pdf')
                    figures1.savefig(figures1_dest)

                    for idx, cell_fig in enumerate(fig_list):
                        cell_fig_dir = os.path.join(destination_dir, 'perCell - singleExp_keff_histFits', f'{tau_tl}s_individualFits')
                        os.makedirs(cell_fig_dir, exist_ok=True)
                        cellFig_dst = os.path.join(cell_fig_dir, f'cell_{idx+1}.pdf')
                        cell_fig.savefig(cellFig_dst)

            txt_output = f"=== Fitting individual cells of {sample_name} to a single exp decay - Results ===\n"

            for tau_tl, (figure, fig_list, keff_list, koff_list, total_count_list) in results_dict.items():
                txt_output += f"\nTimelapse interval = {tau_tl}:\n"
                for idx, ((keff, keff_er), (koff, koff_er), total_count) in enumerate(zip(keff_list, koff_list, total_count_list)):
                    txt_output += f"    - Cell {idx+1}:\n"
                    txt_output += f"        - total counts/tracks = {total_count}\n"
                    # hw_ci = compute_half_width_ci(ci)
                    # hw_koff_ci = compute_half_width_ci(koff_ci)
                    txt_output += f"        - k_eff = {keff:.4f} ± {keff_er:.4f} s⁻¹\n"
                    txt_output += f"        - k_off = {koff:.4f} ± {koff_er:.4f} s⁻¹\n"

            txt_output += f"\nNote: k_off rates are computed by fixing the kb as the value obtained from the global fit."

            # print(txt_output)
            if root_directory or destination_directory:
                txt_destination = os.path.join(destination_dir, 'perCell - singleExp_keff_histFits', f'analysisResults_perCell_fits')
                with open(txt_destination, 'w', encoding='utf-8') as file:
                    file.write(txt_output)

        else: # double exp single cell fits
            results_dict, figures2 = compute_keffs_perCell_double(tracks_dict=input_dict, koff1=koff1_fit_d, koff2=koff2_fit_d, kb=kb_fit_d, tau_int=tau_int)
            if destination_directory or root_directory:
                per_cell_dest = os.path.join(destination_dir, 'perCell - doubleExp_B_histFits')
                os.makedirs(per_cell_dest, exist_ok=True)
                fig2_dest = os.path.join(per_cell_dest,f'Bvalue_perCell_plot.pdf')
                figures2.savefig(fig2_dest)
            
            for tau_tl, (figures1,fig_list,_,_) in results_dict.items():
                if root_directory or destination_directory:
                    figures1_dest = os.path.join(destination_dir, 'perCell - doubleExp_B_histFits', f'{tau_tl}_histFits.pdf')
                    figures1.savefig(figures1_dest)

                    for idx, cell_fig in enumerate(fig_list):
                        cell_fig_dir = os.path.join(destination_dir, 'perCell - doubleExp_B_histFits', f'{tau_tl}s_individualFits')
                        os.makedirs(cell_fig_dir, exist_ok=True)
                        cellFig_dst = os.path.join(cell_fig_dir, f'cell_{idx+1}.pdf')
                        cell_fig.savefig(cellFig_dst)

            txt_output = f"=== Fitting individual cells of {sample_name} to a double exp decay - Results ===\n"
    
            for tau_tl, (_,_, B_list, total_count_list) in results_dict.items():
                txt_output += f"\nTimelapse interval = {tau_tl}s:\n"
                for idx, ((B_val, B_val_err), total_count) in enumerate(zip(B_list, total_count_list)):
                    txt_output += f"    - Cell {idx+1}:\n"
                    txt_output += f"        - total counts/tracks = {total_count}\n"
                    # hw_ci = compute_half_width_ci(ci)
                    txt_output += f"        - percentage of long-lived population = {(B_val*100):.1f} ± {(B_val_err*100):.1f} s⁻¹\n"

            txt_output += f"\nNote: B values are fitted by assuming the same residence times of the two populations obtained from the global fit."

            # print(txt_output)
            if root_directory or destination_directory:
                txt_destination = os.path.join(destination_dir, 'perCell - doubleExp_B_histFits', f'analysisResults_perCell_fits')
                with open(txt_destination, 'w', encoding='utf-8') as file:
                    file.write(txt_output)


def plot_residenceTimes(rt_dict, annotate_data = True, title=None):
    """
    Args: 
    - rt_dict:
        - key: string representing sample name
        - value: two or three element tuple
            - element 1: single residence time float or tuple of two residence times
            - element 2: single float of half width confidence interval or two values if two residence times.
            - element 3: if two residence times: include the fraction of the first residence time.

    - title (str, optional): title of plot if needed

    - example_dict_input: 
    rt_dict = {
    'sample_1': (15, 2, 100), 
    'sample_2': ((3,17), (0.4,3), 30),
    }
    """
    samples = rt_dict.keys()
    sample_positions = np.arange(0.5, len(samples), 1)

    numSamples = len(rt_dict.keys())
    width = 2 + np.sqrt(numSamples) * 1.5
    fig, ax = plt.subplots(figsize=(width, 4.5))
    marker_size = 50  # constant size for all points

    for idx, (key, value) in enumerate(rt_dict.items()):
        if isinstance(value[0], tuple) and len(value[0])==2:
            res_times = value[0]
            errs = value[1]
            frac = value[2]
            for k in range(2):
                ax.errorbar(
                sample_positions[idx],
                res_times[k],
                yerr=errs[k],
                fmt='o',
                markersize=marker_size ** 0.5,
                color='gray',
                ecolor='gray',
                elinewidth=1.5,
                capsize=4
                )

                if annotate_data:
                    # fraction annotation
                    frac1 = frac if k==0 else (100-frac)
                    frac1 = round(frac1, 1) 
                    ax.text(
                        sample_positions[idx] + 0.1, # x offset
                        res_times[k], # same y position as datapoint            
                        str(frac1)+'%',             
                        verticalalignment='center',
                        fontsize=8,
                        color='black'
                    )
        else: 
            res_time = value[0]
            err = value[1]
            ax.errorbar(
                sample_positions[idx],
                res_time,
                yerr=err,
                fmt='o',
                markersize=marker_size ** 0.5,
                color='gray',
                ecolor='gray',
                elinewidth=1.5,
                capsize=4
                )
            if annotate_data:
                frac = 100
                ax.text(
                    sample_positions[idx] + 0.1, # x offset
                    res_time, # same y position as datapoint            
                    str(frac)+'%',             
                    verticalalignment='center',
                    fontsize=8,
                    color='black'
                )
    
    ax.set_xticks(sample_positions)
    ax.set_xticklabels(samples)
    if title:
        ax.set_title(title)
    ax.set_ylabel('Residence Time (s)')

    if annotate_data:
        ax.set_xlim(sample_positions[0] - 0.5, sample_positions[-1] + 0.8)
    else:
        ax.set_xlim(sample_positions[0] - 0.5, sample_positions[-1] + 0.5)
    ymin, ymax = ax.get_ylim()
    ax.set_ylim(0, ymax)
    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

