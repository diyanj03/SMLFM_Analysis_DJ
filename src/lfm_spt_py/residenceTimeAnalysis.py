from scipy.stats import linregress
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.lines import Line2D
from scipy.optimize import curve_fit
from scipy.stats.distributions import t
import os
import pandas as pd
import matplotlib.lines as mlines
import warnings


rgb_list_main = ['crimson', 'darkgreen', 'navy', 'chocolate', 'purple', 'gold', 'cyan']

# plots set xlim_max when the counts reach a 0 or two consecutive 1 and all counts after that are less than or equal to stopPlotConst
# this doesn't affect fitting, only applied for plot aesthetics. 
stopPlotConst = 2 

def compute_rss_bic(model_func, xdata, ydata, popt):
    """
    Compute Bayesian Information Criterion (BIC) for a fitted model. 
    Assumes residuals are gaussian distributed

    Args:
    - model_func (function): The model function used for fitting.
    - xdata (array-like): Input x data.
    - ydata (array-like): Observed y data.
    - popt (tuple/list): Optimised parameters from the fit.

    Returns:
    - bic (float): Bayesian Information Criterion value.
    """
    n = len(ydata)
    k = len(popt)
    residuals = ydata - model_func(xdata, *popt)
    rss = np.sum(residuals**2)
    bic = n * np.log(rss / n) + k * np.log(n) if rss > 0 else np.nan
    return rss, bic

def compute_all_CI(popt, pcov, n_points, alpha=0.05):
    """
    Compute 95% confidence intervals for all fitted parameters.

    Args:
    - popt (array-like): Optimised parameters from curve_fit.
    - pcov (2D array): Covariance matrix from curve_fit.
    - n_points (int): Number of data points used in the fit.
    - alpha (float): Significance level (default=0.05 for 95% CI).

    Returns:
    - ci_list (list of tuples): [(lower1, upper1), (lower2, upper2), ...] for each parameter.
    """
    n_params = len(popt)
    dof = max(0, n_points - n_params)
    if dof == 0 or np.any(np.isnan(pcov)):
        return [(np.nan, np.nan)] * n_params

    t_crit = t.ppf(1 - alpha/2, dof)
    stderr = np.sqrt(np.diag(pcov))

    ci_list = [(param - t_crit * err, param + t_crit * err)
               for param, err in zip(popt, stderr)]
    return ci_list

def compute_half_width_ci(ci_tuple):
    lower, upper = ci_tuple
    return (upper - lower) / 2

def find_first_valid_index(lst):
    n = len(lst)
    for i in range(n):
        if lst[i] == 0:
            if all(x <= stopPlotConst for x in lst[i+1:]):
                return i
        elif lst[i] == 1 and i + 1 < n and lst[i+1] == 1:
            if all(x <= stopPlotConst for x in lst[i+2:]):
                return i+1
    return -1

def extract_counts(tracks_dict):
    """
    Args:
    - tracks_dict (dict): Dictionary where keys represent tau_tl (unit: seconds) and values represent path to csv track files generated by SPT.

    Returns:
    - counts_dict (dict): 
        - keys are tau_interval and values represent a tuple:
        - element 1: histograms (counts) for all track lengths for all datasets with the same tau_interval.
        - element 2: total number of counts (sum of all track lengths) for that tau_interval.
        - element 3: number of FOVs from which counts were extracted for that tau_interval.
    """
    counts_dict = {}
    for tau_interval, tracks_folder in tracks_dict.items():
        numPos_concat = []
        numFOVs = 0
        for track_file_name in os.listdir(tracks_folder):
            if track_file_name.endswith('_positionsFramesIntensity.csv'):
                track_file_path = os.path.join(tracks_folder, track_file_name)
                df = pd.read_csv(track_file_path, comment='#')
                numPos_series = df.iloc[:, 1]
                numPos_concat.extend(numPos_series.tolist())
                numFOVs += 1
        
        if not numPos_concat:
            counts = np.array([], dtype='int64')
            total_counts = 0
        else:
            edges = np.arange(min(numPos_concat) - 0.5, max(numPos_concat) + 0.5 + 0.1, 1)
            counts, _ = np.histogram(numPos_concat, bins=edges)
            counts = np.array(counts).astype('int64')
            total_counts = np.sum(counts) 

        counts_dict[tau_interval] = (counts, total_counts, numFOVs)
    return counts_dict


def compute_keffs(counts_dict,
                  plot_semilog=True, rgb_list=None,
                  scatter_transparency = 0.6, fit_lineWidth = 2,
                  axes_lineWidth=2.0, axisLabel_fontSize=14, tickLabel_fontsize=12,
                  legend_fontSize = 12, legend_titleFontSize = 14,
                  figWidth = 6, figHeight = 5, title_fontSize=15,
                  ylim_min=None, ylim_max=None, xlim_max=None):
    """
    Input raw counts for each timelapse interval for calculating keffs.
    Args:
    - counts_dict (dict): Dictionary where keys represent tau_tl (unit: seconds) and values represent a list of raw counts for each frame.

    Returns:
    - keffs_dict (dict): {tau_tl: A_fit, k_eff_fit, (k_eff_CI_lower, k_eff_CI_upper), keff_sd_err}
    - fig (matplotlib object): plot of the exponential fittings
    - fig2 (matplotlib object): plot of the exponential fittings - survival fraction
    """
    
    keff_dict = {}

    # exponential decay to fit data and model k_eff 
    def exp_decay(x, A, k_eff):
        return A * np.exp(-k_eff * x)
   
    fig, ax = plt.subplots(figsize = (figWidth, figHeight))

    if rgb_list == None:
        rgb_list = rgb_list_main[:len(counts_dict)]

    for i, (tau_tl, value) in enumerate(counts_dict.items()):
        counts = value[0]
        total_counts = np.sum(counts)

        actual_x = np.arange(1, len(counts)+0.01, 1) * tau_tl
        fit_x = np.linspace(min(actual_x), max(actual_x), 200)
        
        counts = np.array(counts)
        valid = counts > 0
        actual_x_valid = actual_x[valid]
        perc_valid = counts[valid]
        log_perc = np.log(perc_valid)
        
        slope, intercept = np.polyfit(actual_x_valid, log_perc, 1)
        k_eff_guess = -slope
        A_guess = np.exp(intercept)
        p0 = [A_guess, k_eff_guess]

        popt, pcov = curve_fit(exp_decay, actual_x, counts, p0=p0)
        A_fit, k_eff_fit = popt
        perr = np.sqrt(np.diag(pcov))
        A_err, k_eff_err = perr
        ci = compute_all_CI(popt, pcov, len(counts))

        keff_dict[tau_tl] = (A_fit, k_eff_fit, ci[1], k_eff_err)
        
        idx_trunc = find_first_valid_index(counts)
        
        perc_surviving = counts / total_counts
        perc_surv_trunc = perc_surviving[:idx_trunc]

        actual_x = actual_x[:idx_trunc]
        fit_x = np.linspace(min(actual_x), max(actual_x), 200)
        y_fit = exp_decay(fit_x, A_fit, k_eff_fit) / total_counts

        ax.scatter(actual_x, perc_surv_trunc, color=rgb_list[i], alpha=scatter_transparency, edgecolor='k', linewidth=0.5, label=f'{tau_tl}s data',zorder=4)
        ax.plot(fit_x, y_fit, color=rgb_list[i], alpha=1.0, linewidth=fit_lineWidth, label=f'{tau_tl}s fit', zorder=3)

    ax.set_xscale('log')
    if plot_semilog:
        ax.set_yscale('log')
        ax.set_xscale('linear')
    
    if ylim_min is not None or ylim_max is not None:
        ax.set_ylim(bottom=ylim_min, top=ylim_max)
    if xlim_max is not None:
        ax.set_xlim(right=xlim_max)

    ax.set_xlabel(r"Time (seconds)")
    ax.set_ylabel("Fraction Surviving")

    ax.set_axisbelow(True)
    ax.grid(True, which='both', linestyle='--', linewidth=0.5, zorder=0)
    ax.tick_params(axis='both', which='major', labelsize=tickLabel_fontsize, width=2, length=6)  # bigger ticks and labels
    ax.xaxis.label.set_size(axisLabel_fontSize)  # x-axis label size
    ax.yaxis.label.set_size(axisLabel_fontSize)  # y-axis label size
    for spine in ax.spines.values():
        spine.set_linewidth(axes_lineWidth)

    handles = []
    tau_tl_values = sorted(counts_dict.keys()) 
    for i, tau_tl in enumerate(tau_tl_values):
        handle = mlines.Line2D([], [], color=rgb_list[i], linestyle='-', linewidth=fit_lineWidth,
                            marker='o', markerfacecolor=rgb_list[i], markeredgecolor='k',
                            markeredgewidth=0.5, alpha=scatter_transparency, markersize=7, label=f'{tau_tl}')
        handles.append(handle)

    ax.legend(handles=handles, title=r'$\tau_{\mathrm{tl}}$ (s)', fontsize=legend_fontSize, title_fontsize=legend_titleFontSize)
    ax.set_title(r'Single-Exponential Decay – $k_{\mathrm{eff}}$ Fit', fontsize=title_fontSize)
    fig.tight_layout()
    return keff_dict, fig

def compute_koff(keff_dict, tau_int,
                 rgb_list=None, lin_scatter_markersize=7.5, lin_force_scatter_black = False,
                 lin_axes_lineWidth=2, lin_axisLabel_fontSize=14, lin_tickLabel_fontsize=12,
                 lin_figWidth = 5, lin_figHeight = 4, lin_force_ylim_upper = None, lin_force_ylim_0 = True, lin_title_fontSize=14,
                 lin_show_error_band=True, lin_confidence_multiplier=3, lin_conf_band_colour=None, lin_conf_band_transparency=0.3,
                 lin_fitColour=None):
    """
    Weighted linear fit of keff * tau_tl vs tau_tl using keff std errors as weights.

    Args:
    - keff_dict (dict): {tau_tl: (A_fit, keff_fit, keff_CI, keff_sd_err)}
    - tau_int (float): camera integration/exposure time in seconds.

    Returns:
    - rates (tuple): (k_off, k_b) in s^-1
    - fit_metrics (tuple): (r_squared, k_off_std_error)
    - fig (matplotlib figure): plot of weighted linear fit
    """

    def linear_func(x, m, c):
        return m * x + c

    tau_tl_list = np.array(list(keff_dict.keys()))
    keff_values = np.array([v[1] for v in keff_dict.values()])
    keff_errors = np.array([v[3] for v in keff_dict.values()])
    keff_upper = (np.array([v[2][1] for v in keff_dict.values()])) * tau_tl_list
    keff_lower = (np.array([v[2][0] for v in keff_dict.values()])) * tau_tl_list

    y = keff_values * tau_tl_list
    y_err = keff_errors * tau_tl_list

    popt, pcov = curve_fit(linear_func, tau_tl_list, y, sigma=y_err, absolute_sigma=True)
    slope, intercept = popt
    koff_se = np.sqrt(np.diag(pcov))[0]
    cb_std_err = np.sqrt(np.diag(pcov))[1]

    k_off = slope
    c_b = intercept
    k_b = c_b / tau_int
    kb_se = cb_std_err/tau_int

    ci_list = compute_all_CI(popt, pcov, len(tau_tl_list))
    ci_list[1] = (np.array(ci_list[1]))/tau_int
    rss,_ = compute_rss_bic(linear_func, tau_tl_list, y, popt)

    t = np.linspace(0, max(tau_tl_list), 200)
    fit = linear_func(t, slope, intercept)

    lower_errors = y - keff_lower  
    upper_errors = keff_upper - y  
    y_err1 = [lower_errors, upper_errors]

    fig, ax = plt.subplots(figsize=(lin_figWidth,lin_figHeight))

    if rgb_list == None:
        rgb_list = rgb_list_main[:len(tau_tl_list)]

    if lin_show_error_band:
        for i in range(len(tau_tl_list)):
            if lin_force_scatter_black:
                marker_col = 'black'
            else: 
                marker_col = rgb_list[i]
            ax.plot(tau_tl_list[i], y[i],
                    'o',
                    markerfacecolor=marker_col,
                    markeredgecolor='black',
                    markersize=lin_scatter_markersize,
                    zorder=4)

        J = np.vstack([t, np.ones_like(t)]).T
        fit_var = np.sum(J @ pcov * J, axis=1)
        fit_std = np.sqrt(fit_var)

        fit_handle, = ax.plot(t, fit, color='black',
                            label=rf'fit: $k_{{\mathrm{{off}}}} = {k_off:.3f}\ \mathrm{{s}}^{{-1}}$',
                            zorder=3)

        if lin_conf_band_colour is None:
            lin_conf_band_colour = 'grey'

        ax.fill_between(t, fit - lin_confidence_multiplier * fit_std,
                        fit + lin_confidence_multiplier * fit_std,
                        color=lin_conf_band_colour, alpha=lin_conf_band_transparency, zorder=2)
    
    else:
        for i in range(len(tau_tl_list)):
            if lin_force_scatter_black:
                marker_col = 'black'
            else: 
                marker_col = rgb_list[i]
                ax.errorbar(
                    tau_tl_list[i], y[i],
                    yerr=[[y_err1[0][i]], [y_err1[1][i]]],
                    fmt='o',
                    markerfacecolor=marker_col,
                    markeredgecolor='black',
                    markersize=lin_scatter_markersize,
                    ecolor='black',
                    zorder=4,
                    capsize=3  # optional: adds small horizontal lines at error bar ends
                )
        
        if lin_fitColour == None:
            lin_fitColour = 'black'
        ax.plot(t, fit, color=lin_fitColour, label=rf'fit: $k_{{\mathrm{{off}}}} = {k_off:.3f}\ \mathrm{{s}}^{{-1}}$', zorder=3)

    ax.set_xlabel(r"$\tau_{\mathrm{tl}}$ (s)")
    ax.set_ylabel(r"$k_{\mathrm{eff}} \, \tau_{\mathrm{tl}}$")

    if lin_force_ylim_0 and lin_force_ylim_upper is None:
        ax.set_ylim(bottom=0)

    if lin_force_ylim_upper is not None and not lin_force_ylim_0:
        ax.set_ylim(top=lin_force_ylim_upper)
    
    if lin_force_ylim_upper is not None and lin_force_ylim_0:
        ax.set_ylim(bottom =0, top=lin_force_ylim_upper)

    ax.set_axisbelow(True)
    ax.grid(True, which='both', linestyle='--', linewidth=0.5, zorder=0)
    ax.tick_params(axis='both', which='major', labelsize=lin_tickLabel_fontsize, width=2, length=6)
    ax.xaxis.label.set_size(lin_axisLabel_fontSize)
    ax.yaxis.label.set_size(lin_axisLabel_fontSize)
    for spine in ax.spines.values():
        spine.set_linewidth(lin_axes_lineWidth)

    ax.set_title(r'Linear - $k_{\mathrm{off}}$ Fit', fontsize=lin_title_fontSize)
    fig.tight_layout()

    return (k_off, k_b), (koff_se, kb_se), (ci_list[0], ci_list[1], rss), fig


def main_sequentialFit(input_dict: dict, tau_int: float, sample_name: str, root_directory: str=None, destination_directory: str=None, **kwargs : dict):
    """
    Performs sequential fitting to compute effective decay rates of SPT counts for each time interval experiment and subsequently computing off and photobleaching rates via a linear fit.
    Below describes the input parameters (including **kwargs arguments).
    parameters attributing to the linear koff fit plot are prefixed with lin
    
    Parameters 
    ----------
    input_dict : dict
        Dictionary where keys represent the interval time (float) and values represent the path to the folder containing timelapse SPT data.
    tau_int : float
        The camera integration or exposure time in seconds.
    sample_name : str
        The name of the sample, used for titling and saving.
    root_directory : str or None, optional
        Path to the root directory for saving outputs (default is None).
    destination_directory : str or None, optional
        Directory name within the root to save results (default is None).
    rgb_list : list or None, optional
        Colours for plotting each `tau_tl` (default is None).
    scatter_transparency : float, optional
        Opaqueness of scatter points, from 0 to 1 (default is 0.6).
    ylim_min : float, optional
        minimum y value for keff plot
    ylim_max : float, optional
        maximum y value for keff plot
    xlim_max : float, optional
        maximum x value for keff plot
    fit_lineWidth : int, optional
        Width of the fit line (default is 2).
    axes_lineWidth : float, optional
        Width of the plot axes lines (default is 1.5).
    axisLabel_fontSize : int, optional
        Font size for the axis labels (default is 14).
    tickLabel_fontsize : int, optional
        Font size for the tick labels (default is 12).
    legend_fontSize : int, optional
        Font size for the legend text (default is 12).
    legend_titleFontSize : int, optional
        Font size for the legend title (default is 14).
    figWidth : int, optional
        Figure width in inches (default is 6).
    figHeight : int, optional
        Figure height in inches (default is 5).
    title_fontSize : int, optional
        Font size for the figure title (default is 15).
    lin_scatter_markersize : float, optional
        Scatter plot marker size (default is 7.5).
    lin_force_scatter_black : bool, optional
        If True, forces scatter points in the linear koff plot to be black (default is False).
    lin_axes_lineWidth : float, optional
        Width of the plot axes lines (default is 2).
    lin_axisLabel_fontSize : int, optional
        Font size for the axis labels (default is 14).
    lin_tickLabel_fontsize : int, optional
        Font size for the tick labels (default is 12).
    lin_figWidth : int, optional
        Figure width in inches (default is 5).
    lin_figHeight : int, optional
        Figure height in inches (default is 4).
    lin_force_ylim_upper : float or None, optional
        Forces the upper y-limit of the plot (default is None).
    lin_force_ylim_0 : bool, optional
        If True, forces the lower y-limit to start at 0 (default is True).
    lin_title_fontSize : int, optional
        Font size for the figure title (default is 14).
    lin_show_error_band : bool, optional
        If True, shows the confidence interval error band (default is True).
    lin_confidence_multiplier : int or float, optional
        Multiplier for the confidence interval width (default is 1).
    lin_conf_band_colour : str or None, optional
        Color of the confidence band (default is None).
    lin_conf_band_transparency : float, optional
        Transparency of the confidence band (default is 0.3).
    lin_fitColour : str or None, optional
        Color of the linear fit line (default is None).
    """
    output95CI = False
    koff_keys = {'rgb_list', 'lin_scatter_markersize', 'lin_force_scatter_black', 'lin_axes_lineWidth', 'lin_axisLabel_fontSize', 'lin_tickLabel_fontsize', 'lin_figWidth', 'lin_figHeight', 'lin_force_ylim_upper', 'lin_force_ylim_0', 'lin_title_fontSize', 'lin_show_error_band', 'lin_confidence_multiplier', 'lin_conf_band_colour', 'lin_conf_band_transparency', 'lin_fitColour'}
    keff_keys = {'rgb_list', 'scatter_transparency', 'fit_lineWidth', 'axes_lineWidth', 'axisLabel_fontSize', 'tickLabel_fontsize', 'legend_fontSize', 'legend_titleFontSize', 'figWidth', 'figHeight', 'title_fontSize', 'ylim_min', 'ylim_max', 'xlim_max'}
    
    allowed_keys = koff_keys | keff_keys
    unknown_keys = set(kwargs) - allowed_keys
    if unknown_keys:
          warnings.warn(f"Ignoring unrecognised keyword argument(s): {', '.join(unknown_keys)}", stacklevel=2)

    first_val = next(iter(input_dict.values()))
    if isinstance(first_val, str):
        counts_dict = extract_counts(input_dict)
    elif (
        isinstance(first_val, tuple) and 
        len(first_val) == 3 and 
        isinstance(first_val[0], (list, np.ndarray))
    ):
        counts_dict = input_dict
    else:
        raise ValueError("Unknown input_dict format")
    
    # defining destinaiton dir 
    if destination_directory:
        destination_dir = os.path.join(destination_directory, f'{sample_name}_residenceTimeAnalysis')
        os.makedirs(destination_dir, exist_ok=True)
    
    if root_directory:
        destination_dir = os.path.join(root_directory, 'results', f'{sample_name}_residenceTimeAnalysis')
        os.makedirs(destination_dir, exist_ok=True)
    
    keff_kwargs = {k: kwargs[k] for k in kwargs if k in keff_keys}
    keff_dict, expFit_fig = compute_keffs(counts_dict, **keff_kwargs)

    koff_kwargs = {k: kwargs[k] for k in kwargs if k in koff_keys}
    rates, (koff_se, kb_se), linFit_metrics, linFit_fig = compute_koff(keff_dict, tau_int, **koff_kwargs)
    
    if destination_directory or root_directory:
    # saving figures
        exp_fit_destination = os.path.join(destination_dir, f'singleExp_keff_histFit.pdf' )
        exp_fit_destination2 = os.path.join(destination_dir, f'singleExp_keff_histFit.png')
        expFit_fig.savefig(exp_fit_destination)
        expFit_fig.savefig(exp_fit_destination2, dpi =600)

        lin_fit_destination = os.path.join(destination_dir, f'singleExp_koff_kb_linFit.pdf')
        linFit_fig.savefig(lin_fit_destination)
        lin_fit_destination2 = os.path.join(destination_dir, f'singleExp_koff_kb_linFit.png')
        linFit_fig.savefig(lin_fit_destination2, dpi=600)

    # txt to save/store results.
    txt_out = f"=== Residence Time Analysis of {sample_name} - Results ===\n\n"
    txt_out += "Inputs per timelapse experiment:\n"
    for tau_tl, (counts, total_counts, numFOVs) in counts_dict.items():
        txt_out += f"-  timelapse {tau_tl}s : numFOVs = {numFOVs} | total_numTracks = {total_counts}\n"
    
    txt_out += "\nk_eff values from fitting each tau_tl distribution to a single-exp decay:\n"
    A_values = []
    for tau_tl, (A_fit, keff_fit, keff_ci, keff_se) in keff_dict.items():
        if output95CI:
            keff_hw_ci = compute_half_width_ci(keff_ci)
            txt_out += f"-  timelapse {tau_tl}s : k_eff = {keff_fit:.4f} ± {keff_hw_ci:.4f} s⁻¹\n"
        else: 
            txt_out += f"-  timelapse {tau_tl}s : k_eff = {keff_fit:.4f} ± {keff_se:.4f} s⁻¹\n"
        A_values.append(A_fit)
    
    txt_out += f"\nCamera integration (exposure time): {tau_int}s\n"

    k_off_ci = compute_half_width_ci(linFit_metrics[0])
    k_b_ci = compute_half_width_ci(linFit_metrics[1])

    
    txt_out += "\nk_off and k_b values from linear reg of k_eff values:\n"
    if output95CI:
        txt_out += f"-  Dissociation rate (k_off): {rates[0]:.4f} ± {k_off_ci:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {(1/rates[0]):.2f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {rates[1]:.4f} ± {k_b_ci:.4f} s⁻¹\n"
    else:
        txt_out += f"-  Dissociation rate (k_off): {rates[0]:.4f} ± {koff_se:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {(1/rates[0]):.2f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {rates[1]:.4f} ± {kb_se:.4f} s⁻¹\n"
    txt_out += f"-  Residual Sum of Squares of linear fit = {linFit_metrics[2]:.4f}\n"

    if output95CI:
        txt_out += "\nNote: '±' denotes the half-width of the 95% confidence interval around the fitted parameter.\n"
    else: 
        txt_out += "\nNote: '±' denotes the s.d. of the fitted parameter.\n"
    print(txt_out)

    # print(f"A_values = {', '.join(str(round(A_value)) for A_value in A_values)}")
    print(f'\n')

    if root_directory or destination_directory:
        txt_destination = os.path.join(destination_dir, 'analysisResults_singleExp_sequentialFit.txt')
        with open(txt_destination, 'w', encoding='utf-8') as file:
            file.write(txt_out)


def get_initial_params(counts_dict, tau_int):
    """
    Input:
    - counts_dict
    - tau_int

    Returns:
    - list of A params guess in order of tl
    - koff guess
    - kb guess
    """
    A_guess_list = []
    keff_values_list = []
    for tau_tl in sorted(counts_dict.keys()):
        value = counts_dict[tau_tl]
        counts = value[0]

        if isinstance(counts, list):
            counts = np.array(counts, dtype=np.int64)  

        actual_x = np.arange(2, len(counts)+1.01, 1) * tau_tl
        fit_x = np.linspace(min(actual_x), max(actual_x), 200)
        
        # guessing initial A and k_eff using log transform fit
        valid = counts > 0
        actual_x_valid = actual_x[valid]
        counts_valid = counts[valid]
        log_counts = np.log(counts_valid)
        slope, intercept = np.polyfit(actual_x_valid, log_counts, 1)
        k_eff_guess = -slope
        A_guess = np.exp(intercept)

        A_guess_list.append(A_guess)
        keff_values_list.append(k_eff_guess)

    tau_tl_sorted = np.array(sorted(counts_dict.keys()))
    keff_values_list = np.array(keff_values_list)

    x = tau_tl_sorted
    y = tau_tl_sorted * keff_values_list

    slope, intercept, _, _, _ = linregress(x, y)
     
    koff_guess = slope
    kb_guess = intercept / tau_int

    return A_guess_list, koff_guess, kb_guess

def get_singleExp_params(counts_dict, tau_int):
    tau_tl_values = sorted(counts_dict.keys())

    t_concat, counts_concat, tau_tl_concat = [], [], []

    for tau_tl in tau_tl_values:
        counts_array, _, _ = counts_dict[tau_tl] # total_counts is unused
        if isinstance(counts_array, list):
            counts_array = np.array(counts_array, dtype=np.int64)
        t_vals = (np.arange(len(counts_array)) + 2) * tau_tl
        t_concat.append(t_vals)
        counts_concat.append(counts_array)
        tau_tl_concat.append(np.full_like(t_vals, tau_tl))

    t_concat = np.concatenate(t_concat)
    counts_concat = np.concatenate(counts_concat)
    tau_tl_concat = np.concatenate(tau_tl_concat)
    
    # model
    def single_exp_decay(x, *params):
        t, tau_tl = x
        n = len(tau_tl_values)
        A_coeffs = params[:n]
        k_off, k_b = params[-2], params[-1]
        y = np.empty_like(t, dtype=float)
        for i, tau in enumerate(tau_tl_values):
            mask = tau_tl == tau
            y[mask] = A_coeffs[i] * np.exp(- (k_b * (tau_int / tau) + k_off) * t[mask])
        return y
    
    
    # curve_fit
    initial_A_guesses, koff_guess, kb_guess = get_initial_params(counts_dict, tau_int)
    
    koff_guess = max(koff_guess, 1e-6)
    kb_guess = max(kb_guess, 1e-6)
    initial_A_guesses = [max(A, 1e-6) for A in initial_A_guesses]

    p0_single = initial_A_guesses + [koff_guess, kb_guess]
    xdata = np.vstack((t_concat, tau_tl_concat))
    ydata = counts_concat

    bounds_single = ([0.0] * len(tau_tl_values) + [0.0, 0.0],
                     [np.inf] * len(tau_tl_values) + [np.inf, np.inf])

    
    popt_single, pcov_single = curve_fit(single_exp_decay, xdata, ydata, p0=p0_single, maxfev=10000, bounds=bounds_single)
    
    A_fits_single = popt_single[:len(tau_tl_values)]
    koff_fit, kb_fit = popt_single[-2], popt_single[-1]

    return A_fits_single, koff_fit, kb_fit




def global_singleExp(counts_dict, tau_int, balance_tl_weights=True, plot_semilog=True, min_residence_time=0.0, cap_residence_time=np.inf,
                     rgb_list=None,
                     scatter_transparency = 0.6, fit_lineWidth = 2,
                     axes_lineWidth=2.0, axisLabel_fontSize=14, tickLabel_fontsize=12,
                     legend_fontSize = 12, legend_titleFontSize = 14,
                     figWidth = 6, figHeight = 5, title_fontSize=15,
                     ylim_min = None, ylim_max = None, xlim_max = None):
    
    tau_tl_values = sorted(counts_dict.keys())
    t_concat, counts_concat, tau_tl_concat, total_counts_concat = [], [], [], []

    for i, tau_tl in enumerate(tau_tl_values):
        counts_array, total_counts, _ = counts_dict[tau_tl]
        if isinstance(counts_array, list):
            counts_array = np.array(counts_array, dtype=np.int64)
        t_vals = (np.arange(len(counts_array)) + 1) * tau_tl # first index in the counts array is at time = tau_tl and not time = 0
        t_concat.append(t_vals)
        counts_concat.append(counts_array)
        tau_tl_concat.append(np.full_like(t_vals, tau_tl))
        total_counts_concat.append(np.full_like(t_vals, total_counts))
        
    t_concat = np.concatenate(t_concat)
    counts_concat = np.concatenate(counts_concat)
    tau_tl_concat = np.concatenate(tau_tl_concat)

    xdata = np.vstack((t_concat, tau_tl_concat))
    ydata = counts_concat

    # model
    def single_exp_decay(x, *params):
        t, tau_tl = x
        n = len(tau_tl_values)
        A_coeffs = params[:n]
        k_off, k_b = params[-2], params[-1]
        y = np.empty_like(t, dtype=float)
        for i, tau in enumerate(tau_tl_values):
            mask = tau_tl == tau # creating a boolean array of same length as tau_tl with True if the value in tau_tl = tau else False.
            y[mask] = A_coeffs[i] * np.exp(- (k_b * (tau_int / tau) + k_off) * t[mask])
        return y
    

    A_guesses,koff_guess,kb_guess = get_singleExp_params(counts_dict, tau_int)

    if min_residence_time == 0:
        koff_upper = np.inf
    else:
        koff_upper = 1/min_residence_time

    if cap_residence_time == np.inf:
        koff_lower = 0.0
    else: 
        koff_lower = 1/cap_residence_time

    bounds_single = ([0.0] * len(tau_tl_values) + [koff_lower, 0.0],
                     [np.inf] * len(tau_tl_values) + [koff_upper, np.inf])

    # p0_single = list(A_guesses) + [koff_guess, kb_guess]
    p0_koff = koff_guess
    if koff_guess < koff_lower:
        p0_koff = koff_lower
    if koff_guess > koff_upper:
        p0_koff = koff_upper
    

    p0_kb = max(kb_guess, 0.0)
    
    p0_single = list(A_guesses) + [p0_koff, p0_kb]

    if balance_tl_weights:
        # Equalise the influence of each tau_tl group
        weights = []
        for tau_tl in tau_tl_values:
            num_points = sum(tau_tl_concat == tau_tl)
            weight = np.sqrt(num_points)  # curve_fit minimises sum((y - f(x)) / sigma)**2
            weights.append(weight)

        tau_to_sigma = {tau: w for tau, w in zip(tau_tl_values, weights)}
        sigma = np.array([tau_to_sigma[tau] for tau in tau_tl_concat], dtype=float)
        popt_single, pcov_single = curve_fit(single_exp_decay, xdata, ydata, p0=p0_single,maxfev=10000,
                                             bounds=bounds_single, sigma=sigma, absolute_sigma=False )
    
    else: 
        popt_single, pcov_single = curve_fit(single_exp_decay, xdata, ydata, p0=p0_single, maxfev=10000, bounds=bounds_single)

    A_fits = popt_single[:len(tau_tl_values)]
    koff_fit, kb_fit = popt_single[-2:]
    koff_se, kb_se = np.sqrt(np.diag(pcov_single))[-2:]

    # metrics - 95% CI + BIC
    rss, bic = compute_rss_bic(single_exp_decay, xdata, ydata, popt_single)
    koff_ci, kb_ci = compute_all_CI(popt_single, pcov_single, len(ydata))[-2:]

    # hist fit plot
    fig, ax = plt.subplots(figsize = (figWidth, figHeight))

    if rgb_list == None:
        rgb_list = rgb_list_main[:len(counts_dict)]

    for i, tau_tl in enumerate(tau_tl_values):
        counts, total_counts, _ = counts_dict[tau_tl]
        counts = np.array(counts)
        perc_surv = counts / total_counts
        idx_trunc = find_first_valid_index(counts)
        perc_surv_trunc = perc_surv[:idx_trunc]

        actual_x = (np.arange(len(perc_surv_trunc)) + 1) * tau_tl
        fit_x = np.linspace(actual_x.min(), actual_x.max(), 200)
        rate = koff_fit + kb_fit * (tau_int / tau_tl)
        y_fit = A_fits[i] * np.exp(-rate * fit_x) / total_counts

        ax.scatter(actual_x, perc_surv_trunc,color=rgb_list[i], alpha=scatter_transparency, edgecolor='k', linewidth=0.5, label=f'{tau_tl}s data',zorder=4)
        ax.plot(fit_x, y_fit, color=rgb_list[i], alpha=1.0, linewidth=fit_lineWidth, label=f'{tau_tl}s fit', zorder=3)
    
    ax.set_xscale('log')
    if plot_semilog:
        ax.set_yscale('log')
        ax.set_xscale('linear')

    if ylim_min is not None or ylim_max is not None:
        ax.set_ylim(bottom=ylim_min, top=ylim_max)
    if xlim_max is not None:
        ax.set_xlim(right=xlim_max)
    
    ax.set_xlabel(r"Time (seconds)")
    ax.set_ylabel("Fraction Surviving")

    ax.set_axisbelow(True)
    ax.grid(True, which='both', linestyle='--', linewidth=0.5, zorder=0)
    ax.tick_params(axis='both', which='major', labelsize=tickLabel_fontsize, width=2, length=6)  # bigger ticks and labels
    ax.xaxis.label.set_size(axisLabel_fontSize)  # x-axis label size
    ax.yaxis.label.set_size(axisLabel_fontSize)  # y-axis label size
    for spine in ax.spines.values():
        spine.set_linewidth(axes_lineWidth)

    handles = []
    tau_tl_values = sorted(counts_dict.keys()) 
    for i, tau_tl in enumerate(tau_tl_values):
        handle = mlines.Line2D([], [], color=rgb_list[i], linestyle='-', linewidth=fit_lineWidth,
                            marker='o', markerfacecolor=rgb_list[i], markeredgecolor='k',
                            markeredgewidth=0.5, alpha=scatter_transparency, markersize=7, label=f'{tau_tl}')
        handles.append(handle)

    ax.legend(handles=handles, title=r'$\tau_{\mathrm{tl}}$ (s)', fontsize=legend_fontSize, title_fontsize=legend_titleFontSize)
    ax.set_title(r'Single-Exponential Decay - Global Fit', fontsize=title_fontSize)
    fig.tight_layout()
    
    return (
        (A_fits, koff_fit, kb_fit),
        (koff_se, kb_se),
        (koff_ci, kb_ci),
        (rss, bic),
        fig
    )

def compute_keffs_perCell_single(tracks_dict, kb_value, tau_int, plot_semilog=True,
                                rgb_list=None, show_singleCell_plots=False,
                                scatter_transparency = 0.6, fit_lineWidth = 2,
                                axes_lineWidth=2.0, axisLabel_fontSize=13, tickLabel_fontsize=12,
                                legend_fontSize = 12, legend_titleFontSize = 14,
                                figWidth = 5, figHeight = 4, title_fontSize=13):
    """
    Args:
    - tracks_dict: keys represent tau_tl and values represent path to folder containing tracks files for that tau_tl.
    - kb_value: tuple of (kb_value, kb_std_error) from global fit
    - tau_int: integration time
    - all plotting kwargs.

    Returns:
    - results_dict: keys represent tau_tl
        - value: 
            - 1st element: list of figures of each cell scatter and fit.
            - 2nd element: list of (keff, keff_err) values 
            - 3rd element: list of (koff, koff_err) values
            - 4th element: list of total_counts
    """
    
    def singleExp_decay(t, A, k_eff):
        return A*np.exp(-(k_eff)*t)
    
    results_dict = {}

    # global_xtau = []
    # global_keffxtau = []
    all_tau_tl = []
    all_koffs = []
    all_keffs = []

    if rgb_list == None:
        rgb_list = rgb_list_main[:len(tracks_dict)]

    for i, (tau_tl, tracks_dir) in enumerate(tracks_dict.items()):
        
        koff_values = []
        keff_values = []
        total_counts = []
        indiv_fig_list = []

        for filename in os.listdir(tracks_dir):
            if not filename.endswith('.csv'):
                continue
            file_path = os.path.join(tracks_dir, filename)
            df = pd.read_csv(file_path, comment = '#')
            list_lengths = df.iloc[:,1]
            bins = np.arange(1.5, max(list_lengths) + 0.6, 1)

            counts,_ = np.histogram(list_lengths, bins=bins)
            counts = np.array(counts)
            total_count = np.sum(counts)

            x_data = np.arange(tau_tl, tau_tl*len(counts)+0.01, tau_tl)
            x_fit = np.linspace(min(x_data), max(x_data), 200)

            valid = counts >0
            x_data_valid = x_data[valid]
            perc_valid = counts[valid]
            log_perc = np.log(perc_valid)

            slope, intercept = np.polyfit(x_data_valid, log_perc, 1)
            k_eff_guess = -slope
            A_guess = np.exp(intercept)
            p0 = [A_guess, k_eff_guess]

            
            popt, pcov = curve_fit(singleExp_decay, x_data, counts, p0=p0)
            A_fit, k_eff_fit = popt
            perr = np.sqrt(np.diag(pcov))

            A_err, k_eff_err = perr
            _,ci = compute_all_CI(popt, pcov, len(counts))

            koff_val = k_eff_fit - (tau_int/tau_tl)*kb_value[0]
            koff_err = np.sqrt(k_eff_err**2 +((tau_int/tau_tl)*kb_value[1])**2)

            # koff_ci = (ci[0] + kb_value[1][0], ci[1] + kb_value[1][1])
            k_off_annot = rf"$k_{{\mathrm{{off}}}} = {koff_val:.3f} \mathrm{{s^{{-1}}}}$"

            perc_surviving = counts / total_count

            # scatter = ax.scatter(x_data, perc_surviving, zorder = 2)
            y_fit = singleExp_decay(x_fit, A_fit, k_eff_fit) / total_count

            # Create individual figure for each cell
            fig_indiv, ax_indiv = plt.subplots(figsize = (figWidth, figHeight))
            
            ax_indiv.scatter(x_data, perc_surviving, color=rgb_list[i], alpha=scatter_transparency, edgecolor='k', linewidth=0.5, label='data',zorder=4)
            ax_indiv.plot(x_fit, y_fit, color=rgb_list[i], alpha=1.0, linewidth=fit_lineWidth, label=f"fit: {k_off_annot}", zorder=3)
            
            ax_indiv.set_xscale('log')
            if plot_semilog:
                ax_indiv.set_yscale('log')
                ax_indiv.set_xscale('linear')

            ax_indiv.set_xlabel(r"Time (seconds)")
            ax_indiv.set_ylabel("Fraction Surviving")

            ax_indiv.set_axisbelow(True)
            ax_indiv.grid(True, which='both', linestyle='--', linewidth=0.5, zorder=0)
            ax_indiv.tick_params(axis='both', which='major', labelsize=tickLabel_fontsize, width=2, length=6)  # bigger ticks and labels
            ax_indiv.xaxis.label.set_size(axisLabel_fontSize)  # x-axis label size
            ax_indiv.yaxis.label.set_size(axisLabel_fontSize)  # y-axis label size
            for spine in ax_indiv.spines.values():
                spine.set_linewidth(axes_lineWidth)

            ax_indiv.set_title(f"Cell={len(koff_values)+1}, $\\tau_{{\\mathrm{{tl}}}}$ = {tau_tl}s, numtracks = {total_count}", fontsize = title_fontSize)
            ax_indiv.legend(fontsize=legend_fontSize)
            fig_indiv.tight_layout()

            if not show_singleCell_plots:
                plt.close(fig_indiv)

            keff_value = (k_eff_fit, k_eff_err)
            keff_values.append(keff_value)

            koff_value = (koff_val, koff_err)
            koff_values.append(koff_value)

            total_counts.append(total_count)
            indiv_fig_list.append(fig_indiv)
        
        # global_xtau.extend([tau_tl] * len(keff_values))
        # global_keffxtau.extend([k_eff * tau_tl for k_eff, _ in keff_values])
        all_tau_tl.extend([tau_tl] * len(koff_values))
        all_koffs.extend([k_off for k_off, _ in koff_values])
        all_keffs.extend([keff for keff,_ in keff_values])

        
        results_dict[tau_tl] = (indiv_fig_list, keff_values, koff_values, total_counts)

    # unique_tau_tl = sorted(set(all_tau_tl))
    # tau_to_pos = {tau: i for i, tau in enumerate(unique_tau_tl)}
    # x_positions = [tau_to_pos[tau] for tau in all_tau_tl]

    # fig2, ax_summary = plt.subplots(figsize=(4, 3))
    # ax_summary.scatter(x_positions, all_keffs, alpha=0.6)
    # ax_summary.set_xticks(range(len(unique_tau_tl)))
    # ax_summary.set_xticklabels([str(tau) for tau in unique_tau_tl], rotation=45)
    # ax_summary.set_xlabel('tau_tl (seconds)')
    # ax_summary.set_ylabel('keff')

    # left_pad = -0.5
    # right_pad = len(unique_tau_tl) - 0.5
    # ax_summary.set_xlim(left_pad, right_pad)

    return results_dict


def global_doubleExp(counts_dict, tau_int, amplitude_multiplier=1000, balance_tl_weights=True, plot_semilog=True, cap_B_value=0.999, min_residence_time=0.0, cap_residence_time=np.inf,
                     rgb_list=None,
                     scatter_transparency = 0.6, fit_lineWidth = 2,
                     axes_lineWidth=2.0, axisLabel_fontSize=14, tickLabel_fontsize=12,
                     legend_fontSize = 12, legend_titleFontSize = 14,
                     figWidth = 6, figHeight = 5, title_fontSize=15,
                     ylim_min = None, ylim_max = None, xlim_max = None):

    tau_tl_values = sorted(counts_dict.keys())
    t_concat, counts_concat, tau_tl_concat, total_counts_concat = [], [], [], []

    for i, tau_tl in enumerate(tau_tl_values):
        counts_array, total_counts, _ = counts_dict[tau_tl]
        if isinstance(counts_array, list):
            counts_array = np.array(counts_array, dtype=np.int64)
        t_vals = (np.arange(len(counts_array)) + 1) * tau_tl
        t_concat.append(t_vals)
        counts_concat.append(counts_array)
        tau_tl_concat.append(np.full_like(t_vals, tau_tl))
        total_counts_concat.append(np.full_like(t_vals, total_counts))

    t_concat = np.concatenate(t_concat)
    counts_concat = np.concatenate(counts_concat)
    tau_tl_concat = np.concatenate(tau_tl_concat)

    xdata = np.vstack((t_concat, tau_tl_concat))
    ydata = counts_concat

    def double_exp_decay(x, *params):
        t, tau_tl  = x
        n = len(tau_tl_values)
        A_coeffs = params[:n]
        B, k_off1, k_off2, k_b = params[-4:]
        y = np.empty_like(t, dtype=float)
        for i, tau in enumerate(tau_tl_values):
            mask = tau_tl == tau
            rate1 = (k_b * (tau_int / tau) + k_off1)
            rate2 = (k_b * (tau_int / tau) + k_off2)
            y[mask] = A_coeffs[i] * (B * np.exp(-rate1 * t[mask]) + (1 - B) * np.exp(-rate2 * t[mask]))
        return y

    # curve_fit
    A_guesses,koff_guess,kb_guess = get_singleExp_params(counts_dict, tau_int)

    upper_bounds = (A_guesses*amplitude_multiplier).tolist()
    
    if min_residence_time == 0:
        koff_upper = np.inf
    else:
        koff_upper = 1/min_residence_time

    if cap_residence_time == np.inf:
        koff_lower = 0.0
    else: 
        koff_lower = 1/cap_residence_time
    
    bounds_double = ([0.0] * len(tau_tl_values) + [(1-cap_B_value), koff_lower, koff_lower, 0.0],
                     upper_bounds + [cap_B_value, koff_upper, koff_upper, np.inf])
    
    # low_bounds = np.array(bounds_double[0], dtype=float)
    # up_bounds = np.array(bounds_double[1], dtype=float)

    # p0_double = A_guesses.tolist() + [0.5, koff_guess, koff_guess*10, kb_guess]

    koff_guess_upper = koff_guess*10
    p0_koff1 = koff_guess
    p0_koff2 = koff_guess*10

    if koff_guess < koff_lower:
        p0_koff1 = koff_lower
        p0_koff2 = koff_lower*10
        if p0_koff2 > koff_upper:
            p0_koff2 = koff_upper

    if koff_guess > koff_upper:
        p0_koff1 = koff_upper/2
        p0_koff2 = koff_upper

    if koff_guess_upper > koff_upper and koff_guess < koff_upper:
        p0_koff2 = koff_upper 
        if p0_koff1 > p0_koff2/2:
            p0_koff1 = p0_koff2/2

    p0_kb = max(kb_guess, 0.0)

    p0_double = A_guesses.tolist() + [0.5, p0_koff1, p0_koff2, p0_kb]

    # print("p0_double:", p0_double)
    # print("lower_bounds:", low_bounds)
    # print("upper_bounds:", up_bounds)

    if balance_tl_weights:
        # Equalise the influence of each tau_tl group
        weights = []
        for tau_tl in tau_tl_values:
            num_points = sum(tau_tl_concat == tau_tl)
            weights.append(np.sqrt(num_points))
        tau_to_sigma = {tau: w for tau, w in zip(tau_tl_values, weights)}
        sigma = np.array([tau_to_sigma[tau] for tau in tau_tl_concat], dtype=float)

        popt_double, pcov_double = curve_fit(
            double_exp_decay, xdata, ydata, p0=p0_double,
            maxfev=20000, bounds=bounds_double,
            sigma=sigma, absolute_sigma=False
        )
    else:
        popt_double, pcov_double = curve_fit(
            double_exp_decay, xdata, ydata, p0=p0_double,
            maxfev=20000, bounds=bounds_double
        )

    A_fits = popt_double[:len(tau_tl_values)]
    B_fit, koff1_fit, koff2_fit, kb_fit = popt_double[-4:]
    B_se, koff1_se, koff2_se, kb_se = np.sqrt(np.diag(pcov_double)[-4:])

    if koff1_fit > koff2_fit:
        koff1_fit, koff2_fit = koff2_fit, koff1_fit
        koff1_se, koff2_se = koff2_se, koff1_se 
        B_fit = 1 - B_fit


    # metrics - 95% CI + BIC
    rss, bic = compute_rss_bic(double_exp_decay, xdata, ydata, popt_double)
    B_ci, koff1_ci, koff2_ci, kb_ci = compute_all_CI(popt_double, pcov_double, len(ydata))[-4:]

    # hist fit 
    fig3, ax3 = plt.subplots(figsize = (figWidth, figHeight))

    if rgb_list == None:
        rgb_list = rgb_list_main[:len(counts_dict)]

    for i, tau_tl in enumerate(tau_tl_values):
        counts, total_counts, _ = counts_dict[tau_tl]
        counts = np.array(counts)
        perc_surv = counts / total_counts
        idx_trunc = find_first_valid_index(counts)
        perc_surv_trunc = perc_surv[:idx_trunc]
        actual_x = (np.arange(len(perc_surv_trunc)) + 1) * tau_tl
        fit_x = np.linspace(actual_x.min(), actual_x.max(), 200)
        rate1 = kb_fit * (tau_int / tau_tl) + koff1_fit
        rate2 = kb_fit * (tau_int / tau_tl) + koff2_fit
        y_fit = A_fits[i] * (B_fit * np.exp(-rate1 * fit_x) + (1 - B_fit) * np.exp(-rate2 * fit_x)) / total_counts

        ax3.scatter(actual_x, perc_surv_trunc,color=rgb_list[i], alpha=scatter_transparency, edgecolor='k', linewidth=0.5, label=f'{tau_tl}s data',zorder=4)
        ax3.plot(fit_x, y_fit, color=rgb_list[i], alpha=1.0, linewidth=fit_lineWidth, label=f'{tau_tl}s fit', zorder=3)
    
    ax3.set_xscale('log')
    if plot_semilog:
        ax3.set_yscale('log')
        ax3.set_xscale('linear')
    
    (bottom_curr, top_curr) = ax3.get_ylim()

    if ylim_min:
        bottom_curr = ylim_min
    if ylim_max:
        top_curr = ylim_max
    
    ax3.set_ylim(bottom=bottom_curr, top=top_curr)

    if xlim_max is not None:
        ax3.set_xlim(right=xlim_max)

    ax3.set_xlabel(r"Time (seconds)")
    ax3.set_ylabel("Fraction Surviving")

    ax3.set_axisbelow(True)
    ax3.grid(True, which='both', linestyle='--', linewidth=0.5, zorder=0)
    ax3.tick_params(axis='both', which='major', labelsize=tickLabel_fontsize, width=2, length=6)  # bigger ticks and labels
    ax3.xaxis.label.set_size(axisLabel_fontSize)  # x-axis label size
    ax3.yaxis.label.set_size(axisLabel_fontSize)  # y-axis label size
    for spine in ax3.spines.values():
        spine.set_linewidth(axes_lineWidth)

    handles = []
    tau_tl_values = sorted(counts_dict.keys()) 
    for i, tau_tl in enumerate(tau_tl_values):
        handle = mlines.Line2D([], [], color=rgb_list[i], linestyle='-', linewidth=fit_lineWidth,
                            marker='o', markerfacecolor=rgb_list[i], markeredgecolor='k',
                            markeredgewidth=0.5, alpha=scatter_transparency, markersize=7, label=f'{tau_tl}')
        handles.append(handle)

    ax3.legend(handles=handles, title=r'$\tau_{\mathrm{tl}}$ (s)', fontsize=legend_fontSize, title_fontsize=legend_titleFontSize)
    ax3.set_title(r'Double-Exponential Decay - Global Fit', fontsize=title_fontSize)
    fig3.tight_layout()

    return (         
    (A_fits, B_fit, koff1_fit, koff2_fit, kb_fit),
    (B_se, koff1_se, koff2_se, kb_se),
    (B_ci, koff1_ci, koff2_ci, kb_ci),
    (rss, bic),
    fig3
    )

def compute_Bval_perCell_double(tracks_dict, koff1, koff2, kb, tau_int,
                                plot_semilog=True, rgb_list=None, show_singleCell_plots=False,
                                scatter_transparency = 0.6, fit_lineWidth = 2,
                                axes_lineWidth=2.0, axisLabel_fontSize=13, tickLabel_fontsize=12,
                                legend_fontSize = 12, legend_titleFontSize = 14,
                                figWidth = 5, figHeight = 4, title_fontSize=13):
    """
    Args:
    - tracks_dict: keys represent tau_tl and values represent path to folder containing tracks files for that tau_tl.
    - koff1 from global fit
    - koff2 from global fit
    - kb from global fit
    - tau_int
    - all plotting kwargs

    Returns:
    - results_dict: keys represent tau_tl
        - value: 
            - first element: list of individual cell figures.
            - second element: list of (B, B_err) values 
            - third element: list of total_count
    - matplotlib figure: distribution of B values across all tau_tl   
    """
    if koff1 > koff2:
        koff1, koff2 = koff2, koff1
    
    def model(tau_tl):
        keff1 = kb*(tau_int / tau_tl) + koff1
        keff2 = kb*(tau_int / tau_tl) + koff2
        def doubleExp_decay(t, A, B):
            return A*(B*np.exp(-(keff1)*t) + (1-B)*np.exp(-(keff2)*t))
        return doubleExp_decay

    results_dict = {}
    all_tau_tl = []
    all_Bs = []

    if rgb_list == None:
        rgb_list = rgb_list_main[:len(tracks_dict)]

    for i, (tau_tl, tracks_dir) in enumerate(tracks_dict.items()):
        
        doubleExp_decay1 = model(tau_tl)

        B_values = []
        total_counts = []
        indiv_fig_list = []

        for filename in os.listdir(tracks_dir):
            if not filename.endswith('.csv'):
                continue
            file_path = os.path.join(tracks_dir, filename)
            df = pd.read_csv(file_path, comment = '#')
            list_lengths = df.iloc[:,1]
            bins = np.arange(1.5, max(list_lengths) + 0.6, 1)

            counts,_ = np.histogram(list_lengths, bins=bins)
            counts = np.array(counts)
            total_count = np.sum(counts)

            x_data = np.arange(tau_tl, tau_tl*len(counts)+0.01, tau_tl)
            x_fit = np.linspace(min(x_data), max(x_data), 200)

            p0 = [1, 0.5]
            
            popt, pcov = curve_fit(
                doubleExp_decay1,
                x_data,
                counts,
                p0=p0,
                bounds=([0, 0.0], [np.inf, 1.0]),
                maxfev=10000  # also add this to avoid convergence issues
                )
            A_fit, B_fit = popt
            perr = np.sqrt(np.diag(pcov))
            A_err, B_err = perr
            _,ci = compute_all_CI(popt, pcov, len(counts))

            B_annot = f"B = {(B_fit*100):.1f}"

            perc_surviving = counts / total_count
            # scatter = ax.scatter(x_data, perc_surviving, zorder = 2)
            y_fit = doubleExp_decay1(x_fit, A_fit, B_fit) / total_count

            fig_indiv, ax_indiv = plt.subplots(figsize = (figWidth, figHeight))

            ax_indiv.scatter(x_data, perc_surviving, color=rgb_list[i], alpha=scatter_transparency, edgecolor='k', linewidth=0.5, label='data',zorder=4)
            ax_indiv.plot(x_fit, y_fit, color=rgb_list[i], alpha=1.0, linewidth=fit_lineWidth, label=f"fit: {B_annot}", zorder=3)
            
            ax_indiv.set_xscale('log')
            if plot_semilog:
                ax_indiv.set_yscale('log')
                ax_indiv.set_xscale('linear')

            ax_indiv.set_xlabel(r"Time (seconds)")
            ax_indiv.set_ylabel("Fraction Surviving")
            
            ax_indiv.set_axisbelow(True)
            ax_indiv.grid(True, which='both', linestyle='--', linewidth=0.5, zorder=0)
            ax_indiv.tick_params(axis='both', which='major', labelsize=tickLabel_fontsize, width=2, length=6)  # bigger ticks and labels
            ax_indiv.xaxis.label.set_size(axisLabel_fontSize)  # x-axis label size
            ax_indiv.yaxis.label.set_size(axisLabel_fontSize)  # y-axis label size
            for spine in ax_indiv.spines.values():
                spine.set_linewidth(axes_lineWidth)

            ax_indiv.set_title(f"Cell={len(B_values)+1}, $\\tau_{{\\mathrm{{tl}}}}$ = {tau_tl}s, numtracks = {total_count}", fontsize = title_fontSize)
            ax_indiv.legend(fontsize=legend_fontSize)
            fig_indiv.tight_layout()
            if not show_singleCell_plots:
                plt.close(fig_indiv)
            
            B_value = (B_fit, B_err)
            B_values.append(B_value)
            total_counts.append(total_count)
            indiv_fig_list.append(fig_indiv)
        
        all_tau_tl.extend([tau_tl] * len(B_values))
        all_Bs.extend([B_val for B_val, _ in B_values])

        results_dict[tau_tl] = (indiv_fig_list, B_values, total_counts)

    return results_dict




def main_globalFit(input_dict : dict, tau_int : int, sample_name : str, root_directory : str = None, destination_directory : str = None, **kwargs : dict):
    """"
    Performs global fitting of SPT counts for each time interval experiment to compute dissociation and photobleaching rates.
    Performs single exponential decay and double exponential decay and chooses the better model.
    Outputs single cell information.
    Below describes the input parameters (including **kwargs arguments). 
    
    Parameters 
    ----------
    input_dict : dict
        Dictionary where keys represent the interval time (float) and values represent the path to the folder containing timelapse SPT data.
    tau_int : float
        The camera integration or exposure time in seconds.
    sample_name : str
        The name of the sample, used for titling and saving.
    root_directory : str or None, optional
        Path to the root directory for saving outputs (default is None).
    destination_directory : str or None, optional
        Custom directory path to save results (default is 'root_dir/results').
    rgb_list : list or None, optional
        Colours for plotting each `tau_tl` (default is None).
    balance_tl_weights : bool, optional
        whether to balance the influence of each tau_tl group in fitting (default is True)
    plot_semilog : bool, optional
        whether to plot yaxis in log scale or not (default is True)
    min_residence_time : float, optional
        a minimum residence time below which values are biologically unfeasible (default is 0.0) 
    cap_residence_time : float, optional
        a maximum residence time above which values are biologically unfeasible (default is np.inf) 
    cap_B_value : float, optional 
        a maximum fraction of one population over the other in doubleExpFit (default is 0.999)
    ylim_min : float, optional
        minimum y value of the plot
    ylim_max : float, optional
        maximum y value of the plot
    xlim_max : float, optional
        maximum x value of the plot
    show_singleCell_plots : bool, optional
        whether to display all single cell plots when calling this (default is False)
    amplitude_multiplier : (float, optional)
        multiple of the singleExp fitted A value to set the cap for the doubleExp A value fitting (default is 1000)
    scatter_transparency : float, optional
        Opaqueness of scatter points, from 0 to 1 (default is 0.6).
    fit_lineWidth : int, optional
        Width of the fit line (default is 2).
    axes_lineWidth : float, optional
        Width of the plot axes lines (default is 2.0).
    axisLabel_fontSize : int, optional
        Font size for the axis labels (default is 14).
    tickLabel_fontsize : int, optional
        Font size for the tick labels (default is 12).
    legend_fontSize : int, optional
        Font size for the legend text (default is 12).
    legend_titleFontSize : int, optional
        Font size for the legend title (default is 14).
    figWidth : int, optional
        Figure width in inches (default is 6).
    figHeight : int, optional
        Figure height in inches (default is 5).
    title_fontSize : int, optional
        Font size for the figure title (default is 15).
    """
    output95CI=False
    first_val = next(iter(input_dict.values()))

    if isinstance(first_val, str):
        counts_dict = extract_counts(input_dict)
        compute_single_cell_fits = True
    elif (
        isinstance(first_val, tuple) and 
        len(first_val) == 3 and 
        isinstance(first_val[0], (list, np.ndarray))
    ):
        counts_dict = input_dict
        compute_single_cell_fits = False
    else:
        raise ValueError("Unknown input_dict format")

    single_keys = {'balance_tl_weights', 'plot_semilog', 'min_residence_time', 'cap_residence_time'}
    double_keys = {'amplitude_multiplier', 'balance_tl_weights', 'plot_semilog', 'cap_B_value', 'min_residence_time', 'cap_residence_time'}
    common_keys = {'rgb_list', 'scatter_transparency', 'fit_lineWidth', 'axes_lineWidth', 'axisLabel_fontSize', 'tickLabel_fontsize', 'legend_fontSize', 'legend_titleFontSize', 'figWidth', 'figHeight', 'title_fontSize', 'ylim_min', 'ylim_max', 'xlim_max'}
    singleCell_single_keys = {'show_singleCell_plots', 'plot_semilog','rgb_list', 'scatter_transparency', 'fit_lineWidth', 'axes_lineWidth', 'axisLabel_fontSize', 'tickLabel_fontsize', 'legend_fontSize', 'legend_titleFontSize', 'figWidth', 'figHeight', 'title_fontSize'}
    singleCell_double_keys = {'show_singleCell_plots', 'plot_semilog','rgb_list', 'scatter_transparency', 'fit_lineWidth', 'axes_lineWidth', 'axisLabel_fontSize', 'tickLabel_fontsize', 'legend_fontSize', 'legend_titleFontSize', 'figWidth', 'figHeight', 'title_fontSize'}

    single_keys.update(common_keys)
    double_keys.update(common_keys)

    allowed_keys = single_keys | double_keys | singleCell_single_keys | singleCell_double_keys
    unknown_keys = set(kwargs) - allowed_keys
    if unknown_keys:
          warnings.warn(f"Ignoring unrecognised keyword argument(s): {', '.join(unknown_keys)}", stacklevel=2)

    single_kwargs = {k: kwargs[k] for k in kwargs if k in single_keys}
    double_kwargs = {k: kwargs[k] for k in kwargs if k in double_keys}
    singleCell_single_kwargs = {k: kwargs[k] for k in kwargs if k in singleCell_single_keys}
    singleCell_double_kwargs = {k: kwargs[k] for k in kwargs if k in singleCell_double_keys}

    (A_fits_s, koff_fit_s, kb_fit_s), (koff_se, kb_se), (koff_ci_s, kb_ci_s), (rss_s, bic_s),fig, = global_singleExp(counts_dict, tau_int, **single_kwargs)
    (A_fits_d, B_fit_d, koff1_fit_d, koff2_fit_d, kb_fit_d), (B_se, koff1_se, koff2_se, kb_se_d), (B_ci_d, koff1_ci_d, koff2_ci_d, kb_ci_d), (rss_d, bic_d), fig3 = global_doubleExp(counts_dict, tau_int, **double_kwargs)

    # save figures
    if destination_directory:
        destination_dir = os.path.join(destination_directory, f'{sample_name}_residenceTimeAnalysis')
        os.makedirs(destination_dir, exist_ok=True)
    
    if root_directory:
        destination_dir = os.path.join(root_directory, 'results', f'{sample_name}_residenceTimeAnalysis')
        os.makedirs(destination_dir, exist_ok=True)

    if destination_directory or root_directory:
        singleExp_histfit_path = os.path.join(destination_dir, 'singleExp_global_histFit.pdf')
        fig.savefig(singleExp_histfit_path)
        singleExp_histfit_path2 = os.path.join(destination_dir, 'singleExp_global_histFit.png')
        fig.savefig(singleExp_histfit_path2, dpi = 600)

        doubleExp_histfit_path = os.path.join(destination_dir, 'doubleExp_global_histFit.pdf')
        fig3.savefig(doubleExp_histfit_path)
        doubleExp_histfit_path2 = os.path.join(destination_dir, 'doubleExp_global_histFit.png')
        fig3.savefig(doubleExp_histfit_path2,dpi=600)


    def half_width_ci(ci):
        return (ci[1] - ci[0]) / 2

    koff_hw_s = half_width_ci(koff_ci_s)
    kb_hw_s = half_width_ci(kb_ci_s)

    B_hw_d = half_width_ci(B_ci_d)
    koff1_hw_d = half_width_ci(koff1_ci_d)
    koff2_hw_d = half_width_ci(koff2_ci_d)
    kb_hw_d = half_width_ci(kb_ci_d)

    # txt output
    txt_out = f"=== Residence Time Analysis of {sample_name} - Results ===\n\n"
    txt_out += "Inputs per timelapse experiment:\n"
    for tau_tl, (counts, total_counts, numFOVs) in counts_dict.items():
        txt_out += f"-  timelapse {tau_tl}s : numFOVs = {numFOVs} | total_numTracks = {total_counts}\n"

    txt_out += f"\nCamera integration (exposure time): {tau_int}s\n"

    # single exp
    txt_out += "\nResults from a global single-exp decay fit:\n"
    if output95CI:
        txt_out += f"-  Dissociation rate (k_off): {koff_fit_s:.4f} ± {koff_hw_s:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff_fit_s:.2f} ± {((koff_hw_s)/((koff_fit_s)**2)):.4f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {kb_fit_s:.4f} ± {kb_hw_s:.4f} s⁻¹\n"
    else:
        txt_out += f"-  Dissociation rate (k_off): {koff_fit_s:.4f} ± {koff_se:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff_fit_s:.2f} ± {((koff_se)/((koff_fit_s)**2)):.4f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {kb_fit_s:.4f} ± {kb_se:.4f} s⁻¹\n" 
    txt_out += f"-  Residual sum of squares: {rss_s:.4f}\n"
    txt_out += f"-  BIC value: {bic_s:.4f}\n"

    # double exp
    txt_out += "\nResults from a global double-exp decay fit:\n"
    if output95CI:
        txt_out += f"-  Fraction of long-lived population (B): {B_fit_d:.3f} ± {B_hw_d:.3f} (95% CI)\n"
        txt_out += f"-  Slow dissociation rate (k_off1): {koff1_fit_d:.4f} ± {koff1_hw_d:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff1_fit_d:.2f} ± {((koff1_hw_d)/((koff1_fit_d)**2)):.4f} s\n"
        txt_out += f"-  Fast dissociation rate (k_off2): {koff2_fit_d:.4f} ± {koff2_hw_d:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff2_fit_d:.2f} ± {((koff2_hw_d)/((koff2_fit_d)**2)):.4f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {kb_fit_d:.4f} ± {kb_hw_d:.4f} s⁻¹\n"
    else:
        txt_out += f"-  Fraction of long-lived population (B): {B_fit_d:.3f} ± {B_se:.3f} (95% CI)\n"
        txt_out += f"-  Slow dissociation rate (k_off1): {koff1_fit_d:.4f} ± {koff1_se:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff1_fit_d:.2f} ± {((koff1_se)/((koff1_fit_d)**2)):.4f} s\n"
        txt_out += f"-  Fast dissociation rate (k_off2): {koff2_fit_d:.4f} ± {koff2_se:.4f} s⁻¹\n"
        txt_out += f"       → Estimated residence time: {1/koff2_fit_d:.2f} ± {((koff2_se)/((koff2_fit_d)**2)):.4f} s\n"
        txt_out += f"-  Photobleaching rate (k_b): {kb_fit_d:.4f} ± {kb_se_d:.4f} s⁻¹\n"
    txt_out += f"-  Residual sum of squares: {rss_d:.4f}\n"
    txt_out += f"-  BIC value: {bic_d:.4f}\n"
    
    if output95CI:
        txt_out += "\nNote: '±' denotes the half-width of the 95% confidence interval around the fitted parameter.\n"
    else:
        txt_out += "\nNote: '±' denotes the s.d. of the fitted parameter.\n"
        
    print(txt_out)

    # print(f"\nsingleExp A values = " + ", ".join([str(A_fits_s[0])] + [str(round(v)) for v in A_fits_s[1:]]))
    # print(f"\ndoubleExp A values = " + ", ".join([str(A_fits_d[0])] + [str(round(v)) for v in A_fits_d[1:]]))
    # print('\n')

    # save .txt
    if root_directory or destination_directory:
        txt_destination = os.path.join(destination_dir, 'analysisResults_globalFits.txt')
        with open(txt_destination, 'w', encoding = 'utf-8') as f:
            f.write(txt_out)

    # single cell analysis
    if compute_single_cell_fits:
        if bic_s < bic_d:
            kb_val = (kb_fit_s, kb_se)
            results_dict = compute_keffs_perCell_single(input_dict, kb_val, tau_int, **singleCell_single_kwargs)
            
            for tau_tl, (fig_list,_,_,_) in results_dict.items():
                if root_directory or destination_directory:
                    for idx, cell_fig in enumerate(fig_list):
                        cell_fig_dir = os.path.join(destination_dir, 'perCell - singleExp_keff_histFits', f'{tau_tl}s_individualFits')
                        os.makedirs(cell_fig_dir, exist_ok=True)
                        cellFig_dst = os.path.join(cell_fig_dir, f'cell_{idx+1}.pdf')
                        cellFig_dst2 = os.path.join(cell_fig_dir, f'cell_{idx+1}.png')
                        cell_fig.savefig(cellFig_dst)
                        cell_fig.savefig(cellFig_dst2, dpi=600)
                        plt.close(cell_fig)

            txt_output = f"=== Fitting individual cells of {sample_name} to a single exp decay - Results ===\n"

            for tau_tl, (fig_list, keff_list, koff_list, total_count_list) in results_dict.items():
                txt_output += f"\nTimelapse interval = {tau_tl}:\n"
                for idx, ((keff, keff_er), (koff, koff_er), total_count) in enumerate(zip(keff_list, koff_list, total_count_list)):
                    txt_output += f"    - Cell {idx+1}:\n"
                    txt_output += f"        - total counts/tracks = {total_count}\n"
                    # hw_ci = compute_half_width_ci(ci)
                    # hw_koff_ci = compute_half_width_ci(koff_ci)
                    txt_output += f"        - k_eff = {keff:.4f} ± {keff_er:.4f} s⁻¹\n"
                    txt_output += f"        - k_off = {koff:.4f} ± {koff_er:.4f} s⁻¹\n"

            txt_output += f"\nNote: k_off rates are computed by fixing the kb as the value obtained from the global fit."

            # print(txt_output)
            if root_directory or destination_directory:
                txt_destination = os.path.join(destination_dir, 'perCell - singleExp_keff_histFits', f'analysisResults_perCell_fits')
                with open(txt_destination, 'w', encoding='utf-8') as file:
                    file.write(txt_output)

        else: # double exp single cell fits
            results_dict = compute_Bval_perCell_double(input_dict, koff1_fit_d, koff2_fit_d, kb_fit_d, tau_int, **singleCell_double_kwargs)
            
            for tau_tl, (fig_list,_,_) in results_dict.items():
                if root_directory or destination_directory:
                    for idx, cell_fig in enumerate(fig_list):
                        cell_fig_dir = os.path.join(destination_dir, 'perCell - doubleExp_B_histFits', f'{tau_tl}s_individualFits')
                        os.makedirs(cell_fig_dir, exist_ok=True)
                        cellFig_dst = os.path.join(cell_fig_dir, f'cell_{idx+1}.pdf')
                        cell_fig.savefig(cellFig_dst)
                        cellFig_dst2 = os.path.join(cell_fig_dir, f'cell_{idx+1}.png')
                        cell_fig.savefig(cellFig_dst2, dpi=600)
                        plt.close(cell_fig)

            txt_output = f"=== Fitting individual cells of {sample_name} to a double exp decay - Results ===\n"
    
            for tau_tl, (_, B_list, total_count_list) in results_dict.items():
                txt_output += f"\nTimelapse interval = {tau_tl}s:\n"
                for idx, ((B_val, B_val_err), total_count) in enumerate(zip(B_list, total_count_list)):
                    txt_output += f"    - Cell {idx+1}:\n"
                    txt_output += f"        - total counts/tracks = {total_count}\n"
                    # hw_ci = compute_half_width_ci(ci)
                    txt_output += f"        - percentage of long-lived population = {(B_val*100):.1f} ± {(B_val_err*100):.1f} s⁻¹\n"

            txt_output += f"\nNote: B values are fitted by assuming the same residence times of the two populations obtained from the global fit."

            # print(txt_output)
            if root_directory or destination_directory:
                txt_destination = os.path.join(destination_dir, 'perCell - doubleExp_B_histFits', f'analysisResults_perCell_fits')
                with open(txt_destination, 'w', encoding='utf-8') as file:
                    file.write(txt_output)


def plot_residenceTimes(rt_dict, annotate_data = True, title=None):
    """
    Args: 
    - rt_dict:
        - key: string representing sample name
        - value: two or three element tuple
            - element 1: single residence time float or tuple of two residence times
            - element 2: single float of half width confidence interval or two values if two residence times.
            - element 3: if two residence times: include the fraction of the first residence time.

    - title (str, optional): title of plot if needed

    - example_dict_input: 
    rt_dict = {
    'sample_1': (15, 2, 100), 
    'sample_2': ((3,17), (0.4,3), 30),
    }
    """
    samples = rt_dict.keys()
    sample_positions = np.arange(0.5, len(samples), 1)

    numSamples = len(rt_dict.keys())
    width = 2 + np.sqrt(numSamples) * 1.5
    fig, ax = plt.subplots(figsize=(width, 4.5))
    marker_size = 50  # constant size for all points

    for idx, (key, value) in enumerate(rt_dict.items()):
        if isinstance(value[0], tuple) and len(value[0])==2:
            res_times = value[0]
            errs = value[1]
            frac = value[2]
            for k in range(2):
                ax.errorbar(
                sample_positions[idx],
                res_times[k],
                yerr=errs[k],
                fmt='o',
                markersize=marker_size ** 0.5,
                color='gray',
                ecolor='gray',
                elinewidth=1.5,
                capsize=4
                )

                if annotate_data:
                    # fraction annotation
                    frac1 = frac if k==0 else (100-frac)
                    frac1 = round(frac1, 1) 
                    ax.text(
                        sample_positions[idx] + 0.1, # x offset
                        res_times[k], # same y position as datapoint            
                        str(frac1)+'%',             
                        verticalalignment='center',
                        fontsize=8,
                        color='black'
                    )
        else: 
            res_time = value[0]
            err = value[1]
            ax.errorbar(
                sample_positions[idx],
                res_time,
                yerr=err,
                fmt='o',
                markersize=marker_size ** 0.5,
                color='gray',
                ecolor='gray',
                elinewidth=1.5,
                capsize=4
                )
            if annotate_data:
                frac = 100
                ax.text(
                    sample_positions[idx] + 0.1, # x offset
                    res_time, # same y position as datapoint            
                    str(frac)+'%',             
                    verticalalignment='center',
                    fontsize=8,
                    color='black'
                )
    
    ax.set_xticks(sample_positions)
    ax.set_xticklabels(samples)
    if title:
        ax.set_title(title)
    ax.set_ylabel('Residence Time (s)')

    if annotate_data:
        ax.set_xlim(sample_positions[0] - 0.5, sample_positions[-1] + 0.8)
    else:
        ax.set_xlim(sample_positions[0] - 0.5, sample_positions[-1] + 0.5)
    ymin, ymax = ax.get_ylim()
    ax.set_ylim(0, ymax)
    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

